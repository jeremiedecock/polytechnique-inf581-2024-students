{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfded97",
   "metadata": {},
   "source": [
    "# Optimization - CEM / ES\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2024-students/main/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2024](https://moodle.polytechnique.fr/course/view.php?id=17108) Lab session #7\n",
    "\n",
    "2019-2024 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f7d60",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab7_optim.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2024-students/main?filepath=lab7_optim.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab7_optim.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2024-students/raw/main/lab7_optim.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228e52b",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous lab we studied a method that allowed us to apply reinforcement learning in continuous state spaces and/or continuous action spaces.\n",
    "We used REINFORCE, a *Policy gradient* method that directly optimize the parametric policy $\\pi_{\\theta}$.\n",
    "The parameter $\\theta$ was iteratively updated toward a local maximum of the total expected reward $J(\\theta)$ using a gradient ascent method:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$$\n",
    "A convenient analytical formulation of $\\nabla_{\\theta}J(\\theta)$ was obtained thanks to the *Policy Gradient theorem*:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\mathbb{E}_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (s,a) Q^{\\pi_\\theta}(s,a) \\right].$$\n",
    "However, gradient ascent methods may have a slow convergence and will only found a local optimum.\n",
    "Moreover, this approach requires an analytical formulation of $\\nabla_\\theta \\log \\pi_\\theta (s,a)$ which is not always known (when something else than a neural networks is used for the agent's policy).\n",
    "\n",
    "Direct Policy Search methods using gradient free optimization procedures like Evolution Strategies or Cross Entropy Method (CEM) are interesting alternatives to Policy Gradient algorithms.\n",
    "They can be successfully applied as long as the $\\pi_\\theta$ policy has no more than few hundreds of parameters.\n",
    "Moreover, these method can solve complex problems that cannot be modeled as Markov Decision Processes.\n",
    "\n",
    "As for previous Reinforcement Learning labs, we will use standard problems provided by Gymnasium suite.\n",
    "Especially, we will try to solve the LunarLander-v2 problem (https://gymnasium.farama.org/environments/box2d/lunar_lander/) which offers both continuous space and action states.\n",
    "\n",
    "As for previous labs, you can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab7_optim.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2024-students/main?filepath=lab7_optim.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-inf581-2024-students/raw/main/lab7_optim.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6f1a1",
   "metadata": {},
   "source": [
    "## Name your work\n",
    "\n",
    "Replace the values in the following dictionary `info`. Your Email must match your class email address. Your Alias will be shown on the public leaderboard (to identify yourself). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "        'Email' : 'firstname.lastname@polytechnique.edu',\n",
    "        'Alias' : 'Anonymous', # (change this in case you want to identify yourself on the leaderboard)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b746d71",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab75a4d1",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `gymnasium`, `numpy`, `pandas`, `seaborn`, `imageio`, `pygame` and `cma`.\n",
    "A complete list of dependencies can be found in the provided [requirements-lab7.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2024-students/master/requirements-lab7.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e596b3",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "Execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468110ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    run_subprocess_command(\"apt install swig xvfb x11-utils\")\n",
    "    run_subprocess_command(\"pip install gymnasium[box2d] pyvirtualdisplay cma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d41d80",
   "metadata": {
    "id": "RJfFvm-4mOI2"
   },
   "outputs": [],
   "source": [
    "#! apt install xvfb x11-utils && pip install gymnasium pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c481586",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a17bf",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, first download the [requirements-lab7.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2024-students/master/requirements-lab7.txt) file and ensure it is located in the same directory as this notebook. Next, run the following command to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "deactivate\n",
    "python3 -m venv env-lab7\n",
    "source env-lab7/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements-lab7.txt\n",
    "```\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "deactivate\n",
    "python3 -m venv env-lab7\n",
    "env\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements-lab7.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb4653",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run INF581 notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker, an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8888:8888 -v \"${PWD}\":/home/jovyan/work jdhp/inf581-lab7:latest\n",
    "```\n",
    "\n",
    "If you encounter an error during the notebook's execution indicating that writing a file is not possible, this issue may stem from the user ID within the container lacking the necessary permissions in the project directory. This problem can be resolved by modifying the directory's permissions, for example, using the command:\n",
    "\n",
    "```bash\n",
    "chmod 777 .\n",
    "rm lab7_*.gif\n",
    "rm lab7_*.png\n",
    "rm *.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3a70c",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "from typing import List, Tuple, Deque, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "#from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb91573",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf9abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27450da1",
   "metadata": {},
   "source": [
    "### Create a Gymnasium rendering wrapper to visualize environments as GIF images within the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e3ee9",
   "metadata": {
    "id": "NAFnXFZcrr-9"
   },
   "outputs": [],
   "source": [
    "# To display GIF images in the notebook\n",
    "\n",
    "import imageio     # To render episodes in GIF images (otherwise there would be no render on Google Colab)\n",
    "                   # C.f. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#bonus-make-a-gif-of-a-trained-agent\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "\n",
    "if is_colab():\n",
    "    import pyvirtualdisplay\n",
    "\n",
    "    _display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                        size=(1400, 900))\n",
    "    _ = _display.start()\n",
    "\n",
    "class RenderWrapper:\n",
    "    def __init__(self, env, force_gif=False):\n",
    "        self.env = env\n",
    "        self.force_gif = force_gif\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "\n",
    "    def render(self):\n",
    "        if not is_colab():\n",
    "            self.env.render()\n",
    "            time.sleep(1./60.)\n",
    "\n",
    "        if is_colab() or self.force_gif:\n",
    "            img = self.env.render()         # Assumes env.render_mode == 'rgb_array'\n",
    "            self.images.append(img)\n",
    "\n",
    "    def make_gif(self, filename=\"render\"):\n",
    "        if is_colab() or self.force_gif:\n",
    "            imageio.mimsave(filename + '.gif', [np.array(img) for i, img in enumerate(self.images) if i%2 == 0], fps=29, loop=0)\n",
    "            return Image(open(filename + '.gif','rb').read())\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, env, force_gif=False):\n",
    "        env.render_wrapper = cls(env, force_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21be878",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement CEM and test it on the CartPole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1532908",
   "metadata": {},
   "source": [
    "Before solving the Lunar Lander environment, we will practice on the (simpler) CartPole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3512d",
   "metadata": {},
   "source": [
    "**Reminder**: a description of the CartPole environment is available at https://gymnasium.farama.org/environments/classic_control/cart_pole/. This environment offers a continuous state space and discrete action space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43163c",
   "metadata": {},
   "source": [
    "An implementation of the CartPole policy is given in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logistic_regression(s, theta):\n",
    "    prob_push_right = sigmoid(np.dot(s, np.transpose(theta)))\n",
    "    return prob_push_right\n",
    "\n",
    "\n",
    "def draw_action(s, theta):\n",
    "    prob_push_right = logistic_regression(s, theta)\n",
    "    r = np.random.rand()\n",
    "    return 1 if r < prob_push_right else 0\n",
    "\n",
    "\n",
    "# Logistic Regression ############################################\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, env):        \n",
    "        self.num_params = env.observation_space.shape[0]\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        return draw_action(state, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72e19a",
   "metadata": {},
   "source": [
    "Optimization algorithms aim to find the minimum of a function. This function is called an \"objective function\".\n",
    "The cell below implements the framework for such a function.\n",
    "Note that in reinforcement learning, by convention the score is a reward to maximize whereas in mathematical optimization the score is a cost to minimize (most optimization libraries like PyCMA used in this lab impose this convention) ; the objective function will therefore return the opposite of the reward as the score of evaluated policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc29e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction:\n",
    "\n",
    "    def __init__(self, env, policy, num_episodes=1, max_time_steps=float('inf'), minimization_solver=True):\n",
    "        self.ndim = policy.num_params  # Number of dimensions of the parameter (weights) space\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.minimization_solver = minimization_solver\n",
    "\n",
    "        self.num_evals = 0\n",
    "\n",
    "        \n",
    "    def eval(self, policy_params, num_episodes=None, max_time_steps=None, render=False):\n",
    "        \"\"\"Evaluate a policy\"\"\"\n",
    "\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "\n",
    "        if max_time_steps is None:\n",
    "            max_time_steps = self.max_time_steps\n",
    "\n",
    "        average_total_rewards = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "\n",
    "            total_rewards = 0.\n",
    "            state, info = self.env.reset()\n",
    "\n",
    "            for t in range(max_time_steps):\n",
    "                if render:\n",
    "                    self.env.render_wrapper.render()\n",
    "\n",
    "                action = self.policy(state, policy_params)\n",
    "                state, reward, done, truncated, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            average_total_rewards += float(total_rewards) / num_episodes\n",
    "\n",
    "            if render:\n",
    "                print(\"Test Episode {0}: Total Reward = {1}\".format(i_episode, total_rewards))\n",
    "\n",
    "        if self.minimization_solver:\n",
    "            average_total_rewards *= -1.\n",
    "\n",
    "        return average_total_rewards   # Optimizers do minimization by default...\n",
    "\n",
    "    \n",
    "    def __call__(self, policy_params, num_episodes=None, max_time_steps=None, render=False):\n",
    "        return self.eval(policy_params, num_episodes, max_time_steps, render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b4fc8",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `cem_uncorrelated` function that search the best $\\theta$ parameters with a Cross Entropy Method. Use the objective function defined above.\n",
    "$\\mathbb{P}$ can be defined as an multivariate normal distribution $\\mathcal{N}\\left( \\boldsymbol{\\mu}, \\boldsymbol{\\sigma^2} \\boldsymbol{\\Sigma} \\right)$ where $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma^2} \\boldsymbol{\\Sigma}$ are vectors i.e. we use one mean and one variance parameters per dimension of $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21a724",
   "metadata": {},
   "source": [
    "**Cross Entropy**\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad\\quad$ $f$: the objective function<br>\n",
    "$\\quad\\quad$ $\\mathbb{P}$: family of distribution<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta}$: initial parameters for the proposal distribution $\\mathbb{P}$<br>\n",
    "\n",
    "**Algorithm parameter**:<br>\n",
    "$\\quad\\quad$ $m$: sample size<br>\n",
    "$\\quad\\quad$ $m_{\\text{elite}}$: number of samples to use to fit $\\boldsymbol{\\theta}$<br>\n",
    "\n",
    "**FOR EACH** iteration<br>\n",
    "$\\quad\\quad$ samples $\\leftarrow \\{ \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\}$ with $\\boldsymbol{x}_i \\sim \\mathbb{P}(\\boldsymbol{\\theta}) ~~ \\forall i \\in 1\\dots m$<br>\n",
    "$\\quad\\quad$ elite $\\leftarrow $ { $m_{\\text{elite}}$ best samples } $\\quad$ (i.e. select best samples according to $f$)<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow $ fit $\\mathbb{P}(\\boldsymbol{\\theta})$ to the elite samples<br>\n",
    "\n",
    "**RETURN** $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_uncorrelated(objective_function,\n",
    "                     mean_array,\n",
    "                     var_array,\n",
    "                     max_iterations=500,\n",
    "                     sample_size=50,\n",
    "                     elite_frac=0.2,\n",
    "                     print_every=10,\n",
    "                     success_score=float(\"inf\"),\n",
    "                     num_evals_for_stop=None,\n",
    "                     hist_dict=None):\n",
    "    \"\"\"Cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        objective_function (function): the function to maximize\n",
    "        mean_array (array of floats): the initial proposal distribution (mean vector)\n",
    "        var_array (array of floats): the initial proposal distribution (variance vector)\n",
    "        max_iterations (int): number of training iterations\n",
    "        sample_size (int): size of population at each iteration\n",
    "        elite_frac (float): rate of top performers to use in update with elite_frac ∈ ]0;1]\n",
    "        print_every (int): how often to print average score\n",
    "        hist_dict (dict): logs\n",
    "    \"\"\"\n",
    "    assert 0. < elite_frac <= 1.\n",
    "\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    # TODO...\n",
    "                \n",
    "    return mean_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2423ac",
   "metadata": {},
   "source": [
    "**Task 2:** train your implementation using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50865fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = LogisticRegression(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=10,\n",
    "                                       max_time_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "init_mean_array = np.random.random(nn_policy.num_params)\n",
    "init_var_array = np.ones(nn_policy.num_params) * 100.\n",
    "\n",
    "theta = cem_uncorrelated(objective_function=objective_function,\n",
    "                         mean_array=init_mean_array,\n",
    "                         var_array=init_var_array,\n",
    "                         max_iterations=30,\n",
    "                         sample_size=50,\n",
    "                         elite_frac=0.1,\n",
    "                         print_every=1,\n",
    "                         success_score=-500,\n",
    "                         num_evals_for_stop=None,\n",
    "                         hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index', columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"var1\", \"var2\", \"var3\", \"var4\"])\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace91738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(title=\"Theta w.r.t training steps\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ad711",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"var1\", \"var2\", \"var3\", \"var4\"]].plot(logy=True, title=\"Variance w.r.t training steps\", figsize=(20, 5))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ce824",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d6d2b",
   "metadata": {},
   "source": [
    "**Task 3:** test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, max_time_steps=200, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab7_ex1_cem_cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65794020",
   "metadata": {},
   "source": [
    "## Exercise 2: implement SAES and solve the CartPole problem with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20bcae",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `saes_1_1` function that search the best $\\theta$ parameters with a (1+1)-SA-ES algorithm. Use the objective function defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56711a36",
   "metadata": {},
   "source": [
    "**(1+1)-SA-ES**\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad\\quad$ $f$: the objective function<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{x}$: initial solution<br>\n",
    "\n",
    "**Algorithm parameter**:<br>\n",
    "$\\quad\\quad$ $\\tau$: self-adaptation learning rate<br>\n",
    "\n",
    "**FOR EACH** generation<br>\n",
    "$\\quad\\quad$ 1. mutation of $\\sigma$ (current individual strategy) : $\\sigma' \\leftarrow \\sigma ~ e^{\\tau \\mathcal{N}(0,1)}$<br>\n",
    "$\\quad\\quad$ 2. mutation of $\\boldsymbol{x}$ (current solution) : $\\boldsymbol{x}' \\leftarrow \\boldsymbol{x} + \\sigma' ~ \\mathcal{N}(0,1)$<br>\n",
    "$\\quad\\quad$ 3. eval $f(\\boldsymbol{x}')$<br>\n",
    "$\\quad\\quad$ 4. survivor selection $\\boldsymbol{x} \\leftarrow \\boldsymbol{x}'$ and $\\sigma \\leftarrow \\sigma'$ if $f(\\boldsymbol{x}') \\leq f(\\boldsymbol{x})$<br>\n",
    "\n",
    "**RETURN** $\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced65d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saes_1_1(objective_function,\n",
    "             x_array,\n",
    "             sigma_array,\n",
    "             max_iterations=500,\n",
    "             tau=None,\n",
    "             print_every=10,\n",
    "             success_score=float(\"inf\"),\n",
    "             num_evals_for_stop=None,\n",
    "             hist_dict=None):\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return x_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533115b3",
   "metadata": {},
   "source": [
    "**Task 2:** train your implementation using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = LogisticRegression(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=10,\n",
    "                                       max_time_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "initial_solution_array = np.random.random(nn_policy.num_params)\n",
    "initial_sigma_array = np.ones(nn_policy.num_params) * 1.\n",
    "\n",
    "theta = saes_1_1(objective_function=objective_function,\n",
    "                 x_array=initial_solution_array,\n",
    "                 sigma_array=initial_sigma_array,\n",
    "                 tau = 0.001,\n",
    "                 max_iterations=1000,\n",
    "                 print_every=100,\n",
    "                 success_score=-500,\n",
    "                 num_evals_for_stop=None,\n",
    "                 hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index', columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"sigma1\", \"sigma2\", \"sigma3\", \"sigma4\"])\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42729eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(title=\"Theta w.r.t training steps\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f32aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"sigma1\", \"sigma2\", \"sigma3\", \"sigma4\"]].plot(logy=True, title=\"Sigma w.r.t training steps\", figsize=(30, 5))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d408e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e520669",
   "metadata": {},
   "source": [
    "**Task 3:** test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80905388",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, max_time_steps=200, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab7_ex2_saes_cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8603746",
   "metadata": {},
   "source": [
    "**Task 4:** try different values of $\\tau$. What happen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c7252",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement a parametric policy $\\pi_\\theta$ for environments having a continuous action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a275c1a",
   "metadata": {},
   "source": [
    "To solve problems having a continuous space, especially to solve the LunarLander problem in the next exercise, we need to define and implement an appropriate parametric policy.\n",
    "For this purpose, we recommend the following neural network:\n",
    "- one hidden layer of 16 units having a ReLu activation function\n",
    "- a tanh activation function on the output layer (be careful on the number of output units)\n",
    "\n",
    "To solve environments with continuous action space like LunarLander with Direct Policy Search methods, a simple procedure that compute the feed forward signal is needed (we don't do back propagation here).\n",
    "A procedure to set/get weights of the network from/to a single vector $\\theta$ will also be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48029065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions ########################################################\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    x_and_zeros = np.array([x, np.zeros(x.shape)])\n",
    "    return np.max(x_and_zeros, axis=0)\n",
    "\n",
    "# Dense Multi-Layer Neural Network ############################################\n",
    "\n",
    "class NeuralNetworkPolicy:\n",
    "\n",
    "    def __init__(self, env, h_size=16):   # h_size = number of neurons on the hidden layer\n",
    "        # Set the neural network activation functions (one function per layer)\n",
    "        self.activation_functions = (relu, tanh)\n",
    "        \n",
    "        # Make a neural network with 1 hidden layer of `h_size` units\n",
    "        weights = (np.zeros([env.observation_space.shape[0] + 1, h_size]),\n",
    "                   np.zeros([h_size + 1, env.action_space.shape[0]]))\n",
    "\n",
    "        self.shape_list = weights_shape(weights)\n",
    "        print(\"Number of parameters per layer:\", self.shape_list)\n",
    "        \n",
    "        self.num_params = len(flatten_weights(weights))\n",
    "        print(\"Number of parameters (neural network weights) to optimize:\", self.num_params)\n",
    "\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        weights = unflatten_weights(theta, self.shape_list)\n",
    "\n",
    "        return feed_forward(inputs=state,\n",
    "                            weights=weights,\n",
    "                            activation_functions=self.activation_functions)\n",
    "\n",
    "\n",
    "def feed_forward(inputs, weights, activation_functions, verbose=False):\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "def weights_shape(weights):\n",
    "    return [weights_array.shape for weights_array in weights]\n",
    "\n",
    "\n",
    "def flatten_weights(weights):\n",
    "    \"\"\"Convert weight parameters to a 1 dimension array (more convenient for optimization algorithms)\"\"\"\n",
    "    nested_list = [weights_2d_array.flatten().tolist() for weights_2d_array in weights]\n",
    "    flat_list = list(itertools.chain(*nested_list))\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def unflatten_weights(flat_list, shape_list):\n",
    "    \"\"\"The reverse function of `flatten_weights`\"\"\"\n",
    "    length_list = [shape[0] * shape[1] for shape in shape_list]\n",
    "\n",
    "    nested_list = []\n",
    "    start_index = 0\n",
    "\n",
    "    for length, shape in zip(length_list, shape_list):\n",
    "        nested_list.append(np.array(flat_list[start_index:start_index+length]).reshape(shape))\n",
    "        start_index += length\n",
    "\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3bfac2",
   "metadata": {},
   "source": [
    "## Exercise 4: solve the LunarLander problem (continuous version) with CEM and SAES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd6e37",
   "metadata": {},
   "source": [
    "**Task 1:** read https://gymnasium.farama.org/environments/box2d/lunar_lander/ to discover the LunarLanderContinuous environment.\n",
    "\n",
    "**Notice:** A reminder of Gymnasium main concepts is available at https://gymnasium.farama.org/content/basic_usage/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4114f",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46184f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "print(\"State space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"State upper bounds:\", env.observation_space.high)\n",
    "print(\"State lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions upper bounds:\", env.action_space.high)\n",
    "print(\"Actions lower bounds:\", env.action_space.low)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d200d54",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic policies (for instance constant actions or randomly drawn actions) to discover the Lunar Lander environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71baa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(150):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    #action = np.array([1., 1.])\n",
    "    action = np.array([-1., -1.])\n",
    "    #action = env.action_space.sample()   # Random policy\n",
    "    \n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab7_ex4_explore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2558aac",
   "metadata": {},
   "source": [
    "**Question 1:** We want to use CEM and SAES to compute the optimal policy for the Lunar Lander environment.\n",
    "What is the size of the search space (number of dimensions) for optimizers knowing that the policy is the one defined in exercise 3 (a neural network of one hidden layer of 16 neurons) and knowing that the State space of the Lunar Lander environment is $\\mathcal{S} = \\mathbb{R}^8$ and its action space is $\\mathcal{A} \\subset \\mathbb{R}^2$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d141f1",
   "metadata": {},
   "source": [
    "**Task 3:** Train the agent using the CEM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=3,\n",
    "                                       max_time_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "init_mean_array = np.random.random(nn_policy.num_params)\n",
    "init_var_array = np.ones(nn_policy.num_params) * 1000.\n",
    "\n",
    "theta = cem_uncorrelated(objective_function=objective_function,\n",
    "                         mean_array=init_mean_array,\n",
    "                         var_array=init_var_array,\n",
    "                         max_iterations=100,\n",
    "                         sample_size=50,\n",
    "                         elite_frac=0.2,\n",
    "                         print_every=1,\n",
    "                         success_score=-200,\n",
    "                         hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e49f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index')\n",
    "ax = df.iloc[:,0].plot(title=\"Average reward\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6225bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,1:66].plot(title=\"Theta w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72900f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,67:].plot(logy=True, title=\"Variance w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d3d22",
   "metadata": {},
   "source": [
    "**Task 4:** check the optimized policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, max_time_steps=500, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab7_ex4_cem_ll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f75adc",
   "metadata": {},
   "source": [
    "**Task 5:** Train the agent using the (1+1)-SA-ES algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42889855",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=5,\n",
    "                                       max_time_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a5151",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "initial_solution_array = np.random.random(nn_policy.num_params)\n",
    "initial_sigma_array = np.ones(nn_policy.num_params) * 1.\n",
    "\n",
    "theta = saes_1_1(objective_function=objective_function,\n",
    "                 x_array=initial_solution_array,\n",
    "                 sigma_array=initial_sigma_array,\n",
    "                 max_iterations=1000,\n",
    "                 print_every=10,\n",
    "                 success_score=-200,\n",
    "                 num_evals_for_stop=None,\n",
    "                 hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index')\n",
    "ax = df.iloc[:,0].plot(title=\"Average reward\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5695244",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,1:66].plot(title=\"Theta w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9fd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,67:].plot(logy=True, title=\"Variance w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4147f7c",
   "metadata": {},
   "source": [
    "**Task 6:** check the optimized policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, max_time_steps=500, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab7_ex4_saes_ll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953af82",
   "metadata": {},
   "source": [
    "## Bonus exercise 1: implement CEM with an alternative *Proposal distribution*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04bedd",
   "metadata": {},
   "source": [
    "Implement CEM with the following alternative *Proposal distribution*: a multivariate normal distribution parametrized by a mean vector and **a covariance matrix** (to use correlations in the search space dimensions).\n",
    "\n",
    "Test it on the CartPole and the LunarLander environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_correlated(objective_function,\n",
    "                   mean_array,\n",
    "                   var_array,\n",
    "                   max_iterations=500,\n",
    "                   sample_size=50,\n",
    "                   elite_frac=0.2,\n",
    "                   print_every=10,\n",
    "                   success_score=float(\"inf\"),\n",
    "                   num_evals_for_stop=None,\n",
    "                   hist_dict=None):\n",
    "    \"\"\"Cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        objective_function (function): the function to maximize\n",
    "        mean_array (array of floats): the initial proposal distribution (mean vector)\n",
    "        var_array (array of floats): the initial proposal distribution (variance vector)\n",
    "        max_iterations (int): number of training iterations\n",
    "        sample_size (int): size of population at each iteration\n",
    "        elite_frac (float): rate of top performers to use in update with elite_frac ∈ ]0;1]\n",
    "        print_every (int): how often to print average score\n",
    "        hist_dict (dict): logs\n",
    "    \"\"\"\n",
    "    assert 0. < elite_frac <= 1.\n",
    "\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    cov_array = np.diag(var_array)\n",
    "\n",
    "    # TODO...\n",
    "                \n",
    "    return mean_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c743f5",
   "metadata": {},
   "source": [
    "### Test on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d81439",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = LogisticRegression(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=10,\n",
    "                                       max_time_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "init_mean_array = np.random.random(nn_policy.num_params)\n",
    "init_var_array = np.ones(nn_policy.num_params) * 100.\n",
    "\n",
    "theta = cem_correlated(objective_function=objective_function,\n",
    "                       mean_array=init_mean_array,\n",
    "                       var_array=init_var_array,\n",
    "                       max_iterations=30,\n",
    "                       sample_size=50,\n",
    "                       elite_frac=0.1,\n",
    "                       print_every=1,\n",
    "                       success_score=-500,\n",
    "                       num_evals_for_stop=None,\n",
    "                       hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62340066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index', columns=[\"score\"] + [f\"mu{i}\" for i in range(1,5)] + [f\"cov{i}{j}\" for i in range(1,5) for j in range(1,5)])\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(title=\"Theta w.r.t training steps\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[f\"cov{i}{j}\" for i in range(1,5) for j in range(1,5)]].plot(logy=True, title=\"Variance w.r.t training steps\", figsize=(20, 5))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2167d",
   "metadata": {},
   "source": [
    "## Bonus exercise 2: test the CMAES algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bb69f",
   "metadata": {},
   "source": [
    "PyCMA: Python implementation of CMA-ES (from Nikolaus Hansen - CMAP).\n",
    "\n",
    "Source code:\n",
    "\n",
    "- http://cma.gforge.inria.fr/cmaes_sourcecode_page.html#python\n",
    "- https://github.com/CMA-ES/pycma\n",
    "- https://pypi.org/project/cma/\n",
    "\n",
    "Official documentation:\n",
    "\n",
    "- http://cma.gforge.inria.fr/apidocs-pycma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=1,\n",
    "                                       max_time_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e392c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x_optimal, es = cma.fmin2(objective_function, x0=np.random.random(nn_policy.num_params), sigma0=10., options={'maxfevals': 1500})\n",
    "theta = x_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "plt.rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "cma.plot();  # shortcut for es.logger.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13fd43",
   "metadata": {},
   "source": [
    "Test the final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function.eval(theta, num_episodes=3, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab7_ex_pycma_ll\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
