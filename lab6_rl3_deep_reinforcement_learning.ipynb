{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4749d3cc",
   "metadata": {},
   "source": [
    "# INF581 Lab6: Deep Reinforcement Learning\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2024-students/main/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2024](https://moodle.polytechnique.fr/course/view.php?id=17108) Lab session #5\n",
    "\n",
    "2019-2024 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cdfc19",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab6_rl3_deep_reinforcement_learning.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2024-students/main?filepath=lab6_rl3_deep_reinforcement_learning.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab6_rl3_deep_reinforcement_learning.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2024-students/raw/main/lab6_rl3_deep_reinforcement_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437194f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous lab, we dealt with reinforcement learning in discrete state and action spaces.\n",
    "To do so, we used methods based on action-value function and especially $Q$-function estimation.\n",
    "The $Q$-function was stored in a table and updated with on- or off- policy algorithms (namely SARSA and $Q$-Learning). \n",
    "\n",
    "Yet, these methods do not scale to large state spaces and especially not to the case of continuous state spaces.\n",
    "To address these issues one can either extend the value-based methods making use of the value-function approximation or directly search in policy spaces.\n",
    "In this lab, we will explore both solutions. \n",
    "\n",
    "The first part of this lab presents the problem to solve: the CartPole environment. \n",
    "\n",
    "In the second part, we will apply value-function approximation methods (namely DQN) to solve the CartPole problem.\n",
    "\n",
    "In the third part of this lab, we will search in a family of parameterized policies $\\pi_\\theta(s, a)$ using a policy gradient method.\n",
    "\n",
    "As for previous labs, you can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab6_rl3_deep_reinforcement_learning.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2024-students/main?filepath=lab6_rl3_deep_reinforcement_learning.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-inf581-2024-students/raw/main/lab6_rl3_deep_reinforcement_learning.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e92df",
   "metadata": {},
   "source": [
    "## Name your work\n",
    "\n",
    "Replace the values in the following dictionary `info`. Your Email must match your class email address. Your Alias will be shown on the public leaderboard (to identify yourself). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "        'Email' : 'firstname.lastname@polytechnique.edu',\n",
    "        'Alias' : 'Anonymous', # (change this in case you want to identify yourself on the leaderboard)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7d953",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b979d",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `torch`, `gymnasium`, `numpy`, `pandas`, `seaborn`, `imageio`, `pygame`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the provided [requirements-lab6.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2024-students/master/requirements-lab6.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdaf83b",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "Execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc839e88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    run_subprocess_command(\"apt install xvfb x11-utils\")\n",
    "    run_subprocess_command(\"pip install gymnasium pyvirtualdisplay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25cb30",
   "metadata": {
    "id": "RJfFvm-4mOI2"
   },
   "outputs": [],
   "source": [
    "#! apt install xvfb x11-utils && pip install gymnasium pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106c1dd",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd945c",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, first download the [requirements-lab6.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2024-students/master/requirements-lab6.txt) file and ensure it is located in the same directory as this notebook. Next, run the following command to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab6\n",
    "source env-lab6/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements-lab6.txt\n",
    "```\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab6\n",
    "env\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements-lab6.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c501c",
   "metadata": {},
   "source": [
    "### Run INF581 notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker, an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8888:8888 -v \"${PWD}\":/home/jovyan/work jdhp/inf581-lab6:latest\n",
    "```\n",
    "\n",
    "If you encounter an error during the notebook's execution indicating that writing a file is not possible, this issue may stem from the user ID within the container lacking the necessary permissions in the project directory. This problem can be resolved by modifying the directory's permissions, for example, using the command:\n",
    "\n",
    "```bash\n",
    "chmod 777 .\n",
    "rm lab6_*.gif\n",
    "rm *.pth\n",
    "rm lab6_reinforce_cartpole_trains_result.png\n",
    "rm lab6_reinforce_cartpole_trains_result_agg.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e141b",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc460dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from typing import List, Tuple, Deque, Optional, Callable\n",
    "# from inf581 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192164e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881d5d7",
   "metadata": {},
   "source": [
    "### Create a Gymnasium rendering wrapper to visualize environments as GIF images within the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397ded1",
   "metadata": {
    "id": "NAFnXFZcrr-9"
   },
   "outputs": [],
   "source": [
    "# To display GIF images in the notebook\n",
    "\n",
    "import imageio     # To render episodes in GIF images (otherwise there would be no render on Google Colab)\n",
    "                   # C.f. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#bonus-make-a-gif-of-a-trained-agent\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "\n",
    "if is_colab():\n",
    "    import pyvirtualdisplay\n",
    "\n",
    "    _display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                        size=(1400, 900))\n",
    "    _ = _display.start()\n",
    "\n",
    "class RenderWrapper:\n",
    "    def __init__(self, env, force_gif=False):\n",
    "        self.env = env\n",
    "        self.force_gif = force_gif\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "\n",
    "    def render(self):\n",
    "        if not is_colab():\n",
    "            self.env.render()\n",
    "            time.sleep(1./60.)\n",
    "\n",
    "        if is_colab() or self.force_gif:\n",
    "            img = self.env.render()         # Assumes env.render_mode == 'rgb_array'\n",
    "            self.images.append(img)\n",
    "\n",
    "    def make_gif(self, filename=\"render\"):\n",
    "        if is_colab() or self.force_gif:\n",
    "            imageio.mimsave(filename + '.gif', [np.array(img) for i, img in enumerate(self.images) if i%2 == 0], fps=29, loop=0)\n",
    "            return Image(open(filename + '.gif','rb').read())\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, env, force_gif=False):\n",
    "        env.render_wrapper = cls(env, force_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3985c2",
   "metadata": {},
   "source": [
    "## Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32188065",
   "metadata": {},
   "source": [
    "### Number of trainings\n",
    "\n",
    "To achieve more representative outcomes at the conclusion of each exercise, we average the results across multiple training sessions. The `NUMBER_OF_TRAININGS` variable specifies the number of training sessions conducted before the results are displayed. \n",
    "\n",
    "We recommend setting a lower value (such as 1 or 2) during the development and testing phases of your implementations. Once you have completed your work and are confident in its functionality, you can increase the number of training sessions to minimize the variance in results. Be aware that a higher number of training sessions will extend the execution time, so adjust this setting in accordance with your computer's capabilities.\n",
    "\n",
    "Additionally, you have the option to assign a specific value to the `NUMBER_OF_TRAININGS` variable for each exercise directly within the cells where the training loop is defined (the `NUMBER_OF_TRAININGS` variable is commented out at the beginning of these cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_TRAININGS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14ca84",
   "metadata": {},
   "source": [
    "### PyTorch device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5f0f5",
   "metadata": {},
   "source": [
    "Retain the cell below to ask PyTorch to utilize the GPU if it's available. For utilizing a GPU on Google Colab, you also have to activate it following the steps outlined [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80405040",
   "metadata": {},
   "source": [
    "Alternatively, you can uncomment the next cell to explicitly instruct PyTorch to train neural networks using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24059bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec950cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch will train neural networks on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c072e3c",
   "metadata": {},
   "source": [
    "## Part 1: Hands on Cart Pole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089de235",
   "metadata": {},
   "source": [
    "As for previous labs, we will use standard environments provided by the Gymnasium, but this time, we will try to solve the CartPole-v1 environment (c.f. https://gymnasium.farama.org/environments/classic_control/cart_pole/) which offers a continuous state space and discrete action space.\n",
    "The Cart Pole task consists in maintaining a pole in a vertical position by moving a cart on which the pole is attached with a joint.\n",
    "No friction is considered.\n",
    "The task is supposed to be solved if the pole stays up-right (within 15 degrees) for 200 steps in average over 100 episodes while keeping the cart position within reasonable bounds.\n",
    "The state is given by $\\{x,\\frac{\\partial x}{\\partial t},\\omega,\\frac{\\partial \\omega}{\\partial t}\\}$ where $x$ is the position of the cart and $\\omega$ is the angle between the pole and vertical position.\n",
    "There are only two possible actions: $a \\in \\{0, 1\\}$ where $a = 0$ means \"push the cart to the LEFT\" and $a = 1$ means \"push the cart to the RIGHT\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a59c5",
   "metadata": {},
   "source": [
    "### Exercise 1: Hands on Cart Pole\n",
    "\n",
    "**Task 1.1:** refer to the following link [CartPole Environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to familiarize yourself with the CartPole environment.\n",
    "\n",
    "**Note:** for a refresher on the key concepts of Gymnasium, you can visit this [Basic Usage Guide](https://gymnasium.farama.org/content/basic_usage/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698214dd",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n.item()\n",
    "\n",
    "print(f\"State space size is: { state_dim }\")\n",
    "print(f\"State upper bounds: { env.observation_space.high }\")\n",
    "print(f\"State lower bounds: { env.observation_space.high }\")\n",
    "print(f\"Action space size is: { action_dim }\")\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1086b9",
   "metadata": {},
   "source": [
    "**Task 1.2:** Run the following cells and check different basic \n",
    "policies (for instance constant actions or randomly drawn actions) to discover the CartPole environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with deep Reinforcement Learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b858b",
   "metadata": {},
   "source": [
    "#### Test the CartPole environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab6_cartpole_left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab6_cartpole_right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4cc7f2",
   "metadata": {},
   "source": [
    "#### Test the CartPole environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for episode_index in range(5):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(70):\n",
    "        env.render_wrapper.render()\n",
    "\n",
    "        if not done:\n",
    "            print(observation)\n",
    "        else:\n",
    "            print(\"x\", end=\"\")\n",
    "        \n",
    "        # TODO...\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab6_cartpole_random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5016f7",
   "metadata": {},
   "source": [
    "## Part 2: Deep value-based reinforcement learning with Deep Q-Networks (DQN)\n",
    "\n",
    "In this part, we will begin our exploration of deep reinforcement learning with Deep Q-Networks (DQN), a famous value-based method.\n",
    "\n",
    "Deep reinforcement learning methods like DQN (Deep Q-Networks) are significant advancements over tabular methods such as Q-Learning because they can handle complex, high-dimensional environments that were previously intractable. While Q-Learning is limited to environments where the state and action spaces are sufficiently small to maintain a table of values, DQN uses neural networks to approximate the Q-value function, allowing it to generalize across similar states and scale to problems with vast state spaces. This enables DQN to learn optimal policies for tasks like video games, robotic control, and other applications where the number of possible states is extraordinarily large.\n",
    "\n",
    "While DQN was designed to tackle large environments like Atari games, the primary focus of this lab is to delve into the underlying algorithms, understand them thoroughly, and evaluate them comprehensively. It's important to note that working with not-so-deep networks captures the essence of deep reinforcement learning, excluding the computational expense. The transition from tabular Q-learning to DQN involves significant implications, primarily due to the ability of DQN to handle high-dimensional state spaces. Moving from DQN to very-deep-DQN is primarily a matter of scale and computational resources. The core principles remain the same, and understanding these principles is the key to mastering reinforcement learning, regardless of the complexity of the network used.\n",
    "For these reasons, in this lab, we will focus on studying the CartPole environment. The CartPole problem is a classic in reinforcement learning, and it provides a simpler and more manageable context for understanding the principles of DQN. The convergence in the CartPole environment is much faster than in Atari games - typically within a minute, as opposed to approximately 10 hours on a well-equipped personal computer for Atari games. This allows us to experiment and iterate more quickly, facilitating a deeper understanding of the algorithms at play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a6cba",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a naive value-based deep reinforcement learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc21f9",
   "metadata": {},
   "source": [
    "Our first step will be to write a naive implementation of a version of Q-Learning, where the Q-function is approximated by a neural network. This approach combines traditional Q-Learning with the power of function approximation provided by neural networks, allowing us to handle environments with large state spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4710c",
   "metadata": {},
   "source": [
    "#### The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733033b9",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}}$ $)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{\\omega} \\leftarrow \\mathbf{\\omega} + \\alpha \\left[ r + \\gamma \\max_{\\mathbf{a}^\\star \\in \\mathcal{A}}\\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'})_{\\mathbf{a}^\\star} - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}} \\right] ~ \\nabla_{\\mathbf{\\omega}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}}$ <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40c7c4",
   "metadata": {},
   "source": [
    "#### PyTorch Refresher and Cheat Sheet\n",
    "\n",
    "In this lab, we will be implementing our deep reinforcement learning algorithms using PyTorch.\n",
    "If you need a refresher, you might find this [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html) helpful. It provides a quick reference for many of the most commonly used PyTorch functions and concepts, and can be a valuable resource as you work through this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc4e41",
   "metadata": {},
   "source": [
    "#### Implement the Q-network\n",
    "\n",
    "The Q-Network is used to approximate the action value function, which gives the expected future reward for taking a particular action in a particular state. The network is trained to minimize the difference between its predicted Q-values and the actual return received.\n",
    "\n",
    "**Task 2.1:** implement the constructor and the `forward` method of the Q-network we will use in our RL agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Q-Network implemented with PyTorch.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        First fully connected layer.\n",
    "    layer2 : torch.nn.Linear\n",
    "        Second fully connected layer.\n",
    "    layer3 : torch.nn.Linear\n",
    "        Third fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the QNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        nn_l1 : int\n",
    "            The number of neurons on the first layer.\n",
    "        nn_l2 : int\n",
    "            The number of neurons on the second layer.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the forward pass of the QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor (state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (Q-values).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d8aaa",
   "metadata": {},
   "source": [
    "#### Implement an inference function\n",
    "\n",
    "**Task 2.2:** Your next assignment is to complete the function below, which will be used to evaluate the performance of an agent in a simulated environment over one or multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q_network_agent(env: gym.Env, q_network: torch.nn.Module, num_episode: int = 1, render: bool = True) -> List[int]:\n",
    "    \"\"\"\n",
    "    Test a naive agent in the given environment using the provided Q-network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to test the agent.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to use for decision making.\n",
    "    num_episode : int, optional\n",
    "        The number of episodes to run, by default 1.\n",
    "    render : bool, optional\n",
    "        Whether to render the environment, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        A list of rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_id in range(num_episode):\n",
    "\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render_wrapper.render()\n",
    "\n",
    "            # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # TODO...\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        print(f\"Episode reward: {episode_reward}\")\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f83c9",
   "metadata": {},
   "source": [
    "**Task 2.3:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c7d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=5)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab6_dqn_naive_untained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397c58f",
   "metadata": {},
   "source": [
    "#### Implement the epsilon greedy function\n",
    "\n",
    "**Task 2.4:** Now, let's proceed to implement the epsilon-greedy strategy, which is a crucial component in balancing exploration and exploitation during the learning process of our reinforcement learning agent. To accomplish this, complete the `__call__` function in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75289069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    An Epsilon-Greedy policy.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        The initial probability of choosing a random action.\n",
    "    epsilon_min : float\n",
    "        The minimum probability of choosing a random action.\n",
    "    epsilon_decay : float\n",
    "        The decay rate for the epsilon value after each action.\n",
    "    env : gym.Env\n",
    "        The environment in which the agent is acting.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-Network used to estimate action values.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(state: np.ndarray) -> np.int64\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "    decay_epsilon()\n",
    "        Decay the epsilon value after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 epsilon_start: float,\n",
    "                 epsilon_min: float,\n",
    "                 epsilon_decay:float,\n",
    "                 env: gym.Env,\n",
    "                 q_network: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of EpsilonGreedy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon_start : float\n",
    "            The initial probability of choosing a random action.\n",
    "        epsilon_min : float\n",
    "            The minimum probability of choosing a random action.\n",
    "        epsilon_decay : float\n",
    "            The decay rate for the epsilon value after each episode.\n",
    "        env : gym.Env\n",
    "            The environment in which the agent is acting.\n",
    "        q_network : torch.nn.Module\n",
    "            The Q-Network used to estimate action values.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "\n",
    "    def __call__(self, state: np.ndarray) -> np.int64:\n",
    "        \"\"\"\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "\n",
    "        If a randomly chosen number is less than epsilon, a random action is chosen.\n",
    "        Otherwise, the action with the highest estimated action value is chosen.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The current state of the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.int64\n",
    "            The chosen action.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the epsilon value after each episode.\n",
    "\n",
    "        The new epsilon value is the maximum of `epsilon_min` and the product of the current \n",
    "        epsilon value and `epsilon_decay`.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184e234",
   "metadata": {},
   "source": [
    "#### Implementing a Learning Rate Scheduler\n",
    "\n",
    "The following cell introduces a PyTorch Learning Rate (LR) scheduler. This scheduler is used for managing and adjusting the learning rate throughout the training process of our agent. It's designed to adjust the learning rate of an optimizer at each epoch, following an exponential decay strategy, but with a lower limit on the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
    "    def __init__(self, optimizer: torch.optim.Optimizer, lr_decay: float, last_epoch: int = -1, min_lr: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of MinimumExponentialLR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            The optimizer whose learning rate should be scheduled.\n",
    "        lr_decay : float\n",
    "            The multiplicative factor of learning rate decay.\n",
    "        last_epoch : int, optional\n",
    "            The index of the last epoch. Default is -1.\n",
    "        min_lr : float, optional\n",
    "            The minimum learning rate. Default is 1e-6.\n",
    "        \"\"\"\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            The learning rates of each parameter group.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            max(base_lr * self.gamma ** self.last_epoch, self.min_lr)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e663b",
   "metadata": {},
   "source": [
    "#### Implementing the Training Function\n",
    "\n",
    "The following function is the final component of our initial agent. It orchestrates the training process, enabling the agent to learn from its interactions with the environment.\n",
    "\n",
    "During each episode, the agent selects actions based on an epsilon-greedy policy, observes the next state and reward from the environment, and updates the weights of the Q-Network based on the observed reward and the maximum predicted Q-value of the next state.\n",
    "\n",
    "**Task 2.5:** complete this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_agent(env: gym.Env,\n",
    "                      q_network: torch.nn.Module,\n",
    "                      optimizer: torch.optim.Optimizer,\n",
    "                      loss_fn: Callable,\n",
    "                      epsilon_greedy: EpsilonGreedy,\n",
    "                      device: torch.device,\n",
    "                      lr_scheduler: _LRScheduler,\n",
    "                      num_episodes: int,\n",
    "                      gamma: float) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # TODO...\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee5657",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# NUMBER_OF_TRAININGS = 20\n",
    "naive_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_naive_agent(env,\n",
    "                                            q_network,\n",
    "                                            optimizer,\n",
    "                                            loss_fn,\n",
    "                                            epsilon_greedy,\n",
    "                                            device,\n",
    "                                            lr_scheduler,\n",
    "                                            num_episodes=150,\n",
    "                                            gamma=0.9)\n",
    "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    naive_trains_result_list[1].extend(episode_reward_list)\n",
    "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "naive_trains_result_df = pd.DataFrame(np.array(naive_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(q_network, \"naive_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de5516",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=naive_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049da848",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", hue=\"agent\", kind=\"line\", data=naive_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddcd800",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"naive_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=5)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab6_dqn_naive_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3bf24",
   "metadata": {},
   "source": [
    "#### Why It Doesn't Work: The Complexity of Deep Reinforcement Learning\n",
    "\n",
    "Our initial deep value-based agent did not converge, primarily due to the three fundamental challenges of value-based deep reinforcement learning:\n",
    "\n",
    "1. **Coverage**: Convergence to the optimal Q-function relies on comprehensive coverage of the state space. However, in the context of deep RL, the state space is often too large to be fully covered. In situations where not all states are sampled due to their vast number, the guarantee of convergence no longer holds.\n",
    "\n",
    "2. **Correlation**: The probability of transitioning to the next state is highly influenced by the current state. This strong correlation can lead to local overfitting and the risk of becoming trapped in a local optimum: the neural network, which approximates the Q-function, may become overly specialized in a small portion of the action-state space and neglect the rest.\n",
    "\n",
    "3. **Convergence**: The \"targets\" used as the truth to be achieved \"move\" during the learning process. For the same prediction (estimation of the value of a state-action pair, i.e., its Q-value), the loss of a given example changes during the learning process (due to *bootstrapping* a main concept of TD-Learning). In other words, DQN tries to minimize a moving target, a target that depends on the model we are learning and optimizing. This can lead to instability and make it difficult for the learning process to converge to an optimal policy.\n",
    "\n",
    "In the following sections, we will explore strategies to address these challenges and improve the performance of our deep reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1313ed4",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Deep Q-Networks v1 (DQN version 2013 with experience replay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40cba4",
   "metadata": {},
   "source": [
    "In 2013, DeepMind made a significant contribution to the field of reinforcement learning with the publication of the paper \"Playing Atari with Deep Reinforcement Learning\" by Volodymyr Mnih and al (https://arxiv.org/abs/1312.5602). This paper marked the introduction of the first version of Deep Q-Networks (DQN).\n",
    "\n",
    "The paper's primary innovation was the development of a technique to decorrelate states in reinforcement learning. This technique, known as *experience replay*, leverages a *replay buffer* to store and sample experiences. The introduction of experience replay greatly enhanced the stability and efficiency of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2803e",
   "metadata": {},
   "source": [
    "#### Experience replay\n",
    "\n",
    "Experience replay is a key technique used in Deep Q-Networks (DQN) to address the issues of correlation.\n",
    "\n",
    "In a typical reinforcement learning setup, an agent learns by interacting with the environment, receiving feedback in the form of rewards, and updating its policy based on this feedback. This process is inherently sequential and the successive states are highly correlated, which can lead to overfitting and instability in learning.\n",
    "\n",
    "Experience replay addresses these issues by storing the agent's experiences, i.e., the tuples of (state, action, reward, next state), in a data structure known as a replay buffer. During the learning process, instead of learning from the most recent experience, the agent randomly samples a batch of experiences from the replay buffer. This random sampling breaks the correlation between successive experiences, leading to more stable and robust learning.\n",
    "\n",
    "Also, by learning from past experiences, the agent can effectively learn from a fixed target, which mitigates the issue of learning from a moving target. This is because the experiences in the replay buffer remain fixed once they are stored, even though the agent's policy continues to evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4861b8",
   "metadata": {},
   "source": [
    "#### DQN v2013 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8442d10c",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $M$<br>\n",
    "\t$\\quad\\quad$ batch size $m$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\mathcal{D}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}$ with random weights $\\mathbf{\\omega}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}}$ $)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ Store transition $(\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'})$ in $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ Sample random batch of transitions $(\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j)$ from $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ Set $y_j = \n",
    "\t\t\\begin{cases} \n",
    "\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\tr_j + \\gamma \\max_{\\mathbf{a}^\\star}\\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'}_j)_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\\end{cases}$<br>\n",
    "\t\t$\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a7b9f",
   "metadata": {},
   "source": [
    "#### Implement the Replay Buffer\n",
    "\n",
    "To incorporate experience replay into the provided naive deep value-based reinforcement learning agent definition, we need to introduce a memory buffer where experiences are stored, and then update the algorithm to sample a random batch of experiences from this buffer to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f076e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A Replay Buffer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : collections.deque\n",
    "        A double-ended queue where the transitions are stored.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    add(state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool)\n",
    "        Add a new transition to the buffer.\n",
    "    sample(batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "        Sample a batch of transitions from the buffer.\n",
    "    __len__()\n",
    "        Return the current size of the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes a ReplayBuffer instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of transitions that can be stored in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool):\n",
    "        \"\"\"\n",
    "        Add a new transition to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The state vector of the added transition.\n",
    "        action : np.int64\n",
    "            The action of the added transition.\n",
    "        reward : float\n",
    "            The reward of the added transition.\n",
    "        next_state : np.ndarray\n",
    "            The next state vector of the added transition.\n",
    "        done : bool\n",
    "            The final state of the added transition.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "            A batch of `batch_size` transitions.\n",
    "        \"\"\"\n",
    "        # Here, `random.sample(self.buffer, batch_size)`\n",
    "        # returns a list of tuples `(state, action, reward, next_state, done)`\n",
    "        # where:\n",
    "        # - `state`  and `next_state` are numpy arrays\n",
    "        # - `action` and `reward` are floats\n",
    "        # - `done` is a boolean\n",
    "        #\n",
    "        # `states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))`\n",
    "        # generates 5 tuples `state`, `action`, `reward`, `next_state` and `done`, each having `batch_size` elements.\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb9832",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "The training function of our initial deep value-based agent needs to be modified to incorporate the use of the replay buffer effectively.\n",
    "\n",
    "1. **Store Experiences**: After the agent takes an action and receives a reward and the next state from the environment, store this experience in the replay buffer.\n",
    "\n",
    "2. **Sample Experiences**: Instead of using the most recent experience to update the agent's policy, randomly sample a batch of experiences from the replay buffer.\n",
    "\n",
    "3. **Compute Loss and Update Weights**: Use the sampled experiences to compute the loss and update the weights of the Q-Network.\n",
    "\n",
    "4. **Handle Terminal States**: If the 'done' flag of an experience is True, indicating a terminal state, make sure to adjust the target Q-value to be just the received reward. This is because there are no future rewards possible after a terminal state.\n",
    "\n",
    "**Task 3.1:** complete the `train_dqn1_agent` to use the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00424715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn1_agent(env: gym.Env,\n",
    "                     q_network: torch.nn.Module,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     loss_fn: Callable,\n",
    "                     epsilon_greedy: EpsilonGreedy,\n",
    "                     device: torch.device,\n",
    "                     lr_scheduler: _LRScheduler,\n",
    "                     num_episodes: int,\n",
    "                     gamma: float,\n",
    "                     batch_size: int,\n",
    "                     replay_buffer: ReplayBuffer) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # TODO...\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54080856",
   "metadata": {},
   "source": [
    "#### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# NUMBER_OF_TRAININGS = 20\n",
    "dqn1_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "    \n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "    \n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn1_agent(env,\n",
    "                                           q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=150,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer)\n",
    "    dqn1_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn1_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn1_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn1_trains_result_df = pd.DataFrame(np.array(dqn1_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn1_trains_result_df[\"agent\"] = \"DQN 2013\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, \"dqn1_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0b411",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dqn1_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa314e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn1_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9019c5",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a473fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dqn1_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab6_dqn1_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a61a0",
   "metadata": {},
   "source": [
    "### Exercise 4: Implement Deep Q-Networks v2 (DQN version 2015) with *infrequent weight updates*\n",
    "\n",
    "In 2015, DeepMind further advanced the field of reinforcement learning with the publication of the paper \"Human-level control through deep reinforcement learning\" by Volodymyr Mnih and colleagues (https://www.nature.com/articles/nature14236). This work introduced the second version of Deep Q-Networks (DQN).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2024-students/master/lab6_dqn_nature_journal.jpg\" width=\"200px\" />\n",
    "\n",
    "The key contribution of this paper was the introduction of a method to stabilize the learning process by infrequently updating the target weights. This technique, known as *infrequent updates of target weights*, significantly improved the stability of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268807de",
   "metadata": {},
   "source": [
    "#### Infrequent weight updates\n",
    "\n",
    "Infrequent weight updates, also known as the use of a target network, is a technique used in Deep Q-Networks (DQN) to address the issue of learning from a moving target.\n",
    "\n",
    "In a typical DQN setup, there are two neural networks: the Q-network and the target network. The Q-network is used to predict the Q-values and is updated at every time step. The target network is used to compute the target Q-values for the update, and its weights are updated less frequently, typically every few thousand steps, by copying the weights from the Q-network.\n",
    "\n",
    "The idea behind infrequent weight updates is to stabilize the learning process by keeping the target Q-values fixed for a number of steps. This mitigates the issue of learning from a moving target, as the target Q-values remain fixed between updates.\n",
    "\n",
    "Without infrequent weight updates, both the predicted and target Q-values would change at every step, which could lead to oscillations and divergence in the learning process. By introducing a delay between updates of the target Q-values, the risk of such oscillations is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cffa927",
   "metadata": {},
   "source": [
    "#### DQN v2015 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbed5c2",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $M$<br>\n",
    "\t$\\quad\\quad$ batch size $m$<br>\n",
    "\t$\\quad\\quad$ target network update frequency $\\tau$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\mathcal{D}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}$ with random weights $\\mathbf{\\omega}$<br>\n",
    "<b>Initialize</b> target action-value function $\\hat{Q}$ with weights $\\mathbf{\\omega_2} = \\mathbf{\\omega}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega_1}}$ $)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ Store transition $(\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'})$ in $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ Sample random batch of transitions $(\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j)$ from $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ For each $j$, set $y_j = \n",
    "\t\t\\begin{cases} \n",
    "\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\tr_j + \\gamma \\max_{\\mathbf{a}^\\star} \\hat{Q}_{\\mathbf{\\omega_2}} (\\mathbf{s'}_j)_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\\end{cases}$<br>\n",
    "\t\t$\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega_1}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega_1}$<br>\n",
    "\t\t$\\quad\\quad$ Every $\\tau$ steps reset $\\hat{Q}_{\\mathbf{\\omega_2}}$ to $\\hat{Q}_{\\mathbf{\\omega_1}}$, i.e., set $\\mathbf{\\omega_2} \\leftarrow \\mathbf{\\omega_1}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6672926",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "To incorporate the use of infrequent weight updates in the training function, you would need to make the following modifications:\n",
    "\n",
    "1. **Update the Target Network Infrequently**: Instead of updating the weights of the target network at every time step, update them less frequently, for example, every few thousand steps. The weights of the target network are updated by copying the weights from the Q-network.\n",
    "\n",
    "2. **Compute Target Q-values with the Target Network**: When computing the target Q-values for the update, use the target network instead of the Q-network. This ensures that the target Q-values remain fixed between updates, which stabilizes the learning process.\n",
    "\n",
    "**Task 4.1:** complete the `train_dqn2_agent` to apply infrequent weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn2_agent(env: gym.Env,\n",
    "                     q_network: torch.nn.Module,\n",
    "                     target_q_network: torch.nn.Module,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     loss_fn: Callable,\n",
    "                     epsilon_greedy: EpsilonGreedy,\n",
    "                     device: torch.device,\n",
    "                     lr_scheduler: _LRScheduler,\n",
    "                     num_episodes: int,\n",
    "                     gamma: float,\n",
    "                     batch_size: int,\n",
    "                     replay_buffer: ReplayBuffer,\n",
    "                     target_q_network_sync_period: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the q_network weights with a batch of experiences from the buffer\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # TODO...\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # Update the target q-network\n",
    "\n",
    "            # Every few training steps (e.g., every 100 steps), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            # TODO...\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce367828",
   "metadata": {},
   "source": [
    "#### Train it\n",
    "\n",
    "In order to test this new implementation, we needs de adapt the following cell to instantiate and initialize the two neural networks.\n",
    "\n",
    "**Task 4.2:** complete the following cell to make the two Q-Networks. Initialize a target network that has the same architecture as the Q-network. The weights of the target network are initially copied from the Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e59865",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# NUMBER_OF_TRAININGS = 20\n",
    "dqn2_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn2_agent(env,\n",
    "                                           q_network,\n",
    "                                           target_q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=150,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer,\n",
    "                                           target_q_network_sync_period=30)\n",
    "    dqn2_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn2_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn2_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn2_trains_result_df = pd.DataFrame(np.array(dqn2_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn2_trains_result_df[\"agent\"] = \"DQN 2015\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, \"dqn2_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be5702e",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dqn2_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn2_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22744ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5779d",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d74dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dqn2_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab6_dqn2_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259b560",
   "metadata": {},
   "source": [
    "## Part 3: Deep policy-based reinforcement learning with Monte Carlo Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c672355",
   "metadata": {},
   "source": [
    "In the previous part, we explored DQN, a value-based method that can be used to solve Reinforcement Learning problems having large state spaces.\n",
    "Now, we will solve the CartPole environment using a policy gradient method which directly searchs in a family of parameterized policies $\\pi_\\theta$ for the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb822072",
   "metadata": {},
   "source": [
    "### The Policy Gradient theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3b781",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Policy Gradient methods performs gradient ascent in the policy space so that the total return is maximized.\n",
    "We will restrict our work to episodic tasks, *i.e.* tasks that have a starting states and last for a finite and fixed number of steps $T$, called horizon. \n",
    "\n",
    "More formally, we define an optimization criterion that we want to maximize:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=1}^T r(s_t,a_t)\\right],$$\n",
    "\n",
    "where $\\mathbb{E}_{\\pi_\\theta}$ means $a \\sim \\pi_\\theta(s,.)$ and $T$ is the horizon of the episode.\n",
    "In other words, we want to maximize the value of the starting state: $V^{\\pi_\\theta}(s)$.\n",
    "The policy gradient theorem tells us that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\mathbb{E}_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (s,a) ~ Q^{\\pi_\\theta}(s,a) \\right],\n",
    "$$\n",
    "\n",
    "where the $Q$-function is defined as:\n",
    "\n",
    "$$Q^{\\pi_\\theta}(s,a) = \\mathbb{E}^{\\pi_\\theta} \\left[\\sum_{t=1}^T r(s_t,a_t)|s=s_1, a=a_1\\right].$$\n",
    "\n",
    "The policy gradient theorem is particularly effective because it allows gradient computation without needing to understand the system's dynamics, as long as the $Q$-function for the current policy is computable. By simply applying the policy and observing the one-step transitions, sufficient information is gathered. Implementing a stochastic gradient ascent and substituting $Q^{\\pi_\\theta}(s_t,a_t)$ with a Monte Carlo estimate $R_t = \\sum_{t'=t}^T r(s_{t'},a_{t'})$ for a single trajectory, we derive the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59028d03",
   "metadata": {},
   "source": [
    "The REINFORCE algorithm, introduced by Williams in 1992, is a Monte Carlo policy gradient method. It updates the policy in the direction that maximizes rewards, using full-episode returns as an unbiased estimate of the gradient. Each step involves generating an episode using the current policy, computing the gradient estimate, and updating the policy parameters. This algorithm is simple yet powerful, and it's particularly effective in environments where the policy gradient is noisy or the dynamics are complex.\n",
    "\n",
    "For further reading and a deeper understanding, refer to Williams' seminal paper (https://link.springer.com/article/10.1007/BF00992696) and the comprehensive text on reinforcement learning by Richard S. Sutton and Andrew G. Barto: \"Reinforcement Learning: An Introduction\", chap.13 (http://incompleteideas.net/book/RLbook2020.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247617e",
   "metadata": {},
   "source": [
    "Here is the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e25958",
   "metadata": {},
   "source": [
    "### Monte Carlo policy gradient (REINFORCE)\n",
    "\n",
    "<b>REQUIRE</b> <br>\n",
    "$\\quad$ A differentiable policy $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ A learning rate $\\alpha \\in \\mathbb{R}^+$ <br>\n",
    "<b>INITIALIZATION</b> <br>\n",
    "$\\quad$ Initialize parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ <br>\n",
    "<br>\n",
    "<b>FOR EACH</b> episode <br>\n",
    "$\\quad$ Generate full trace $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_1, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_T, \\boldsymbol{s}_T \\}$ following $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ <b>FOR</b> $~ t=0,\\dots,T-1$ <br>\n",
    "$\\quad\\quad$ $G \\leftarrow \\sum_{k=t}^{T-1} r_k$ <br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha ~ \\underbrace{G ~ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{a}_t|\\boldsymbol{s}_t)}_{\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})}$ <br>\n",
    "<br>\n",
    "<b>RETURN</b> $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa539b7",
   "metadata": {},
   "source": [
    "### Exercise 5: REINFORCE for discrete action spaces (Cartpole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457380b6",
   "metadata": {},
   "source": [
    "#### Policy Implementation\n",
    "\n",
    "We will implement a stochastic policy to control the cart using a simple one-layer neural network. Given the simplicity of the problem, a single layer will suffice. We will not incorporate a bias term in this layer.\n",
    "\n",
    "This neural network will output the probabilities of each possible action (in this case, there are only two actions: \"push left\" or \"push right\") given the input vector $s$ (the 4-dimensional state vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9bb495",
   "metadata": {},
   "source": [
    "**Task 5.1**: Implement the `PolicyNetwork`  defined as follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6ac49",
   "metadata": {},
   "source": [
    "The network takes an input tensor representing the state of the environment and outputs a tensor of action probabilities.\n",
    "The network has the following components:\n",
    "\n",
    "- `layer1`: This is a linear (fully connected) layer that takes `n_observations` as input and outputs `n_actions`. It does not include a bias term.\n",
    "\n",
    "- `forward` method: This method defines the forward pass of the network. It takes a state tensor as input and returns a tensor of action probabilities. It first applies the linear layer to the input state tensor to get the logits (the raw, unnormalized scores for each action), and then applies the softmax function to the logits to get the action probabilities. The softmax function ensures that the action probabilities are positive and sum to 1, so they can be interpreted as probabilities.\n",
    "\n",
    "This network is quite simple and may not perform well on complex tasks with large state or action spaces. However, it can be a good starting point for simple reinforcement learning tasks, and can be easily extended with more layers or different types of layers (such as convolutional layers for image inputs) to handle more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network used as a policy for the REINFORCE algorithm.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        A fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(state: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the PolicyNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of PolicyNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "\n",
    "    def forward(self, state_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the probability of each action for the given state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_tensor : torch.Tensor\n",
    "            The input tensor (state).\n",
    "            The shape of the tensor should be (N, dim),\n",
    "            where N is the number of states vectors in the batch\n",
    "            and dim is the dimension of state vectors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (the probability of each action for the given state).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983977a",
   "metadata": {},
   "source": [
    "**Task 5.2**: Complete the `sample_discrete_action` function. This function is used to sample a discrete action based on a given state and a policy network. It first converts the state into a tensor and passes it through the policy network to get the parameters of the action probability distribution. Then, it creates a categorical distribution from these parameters and samples an action from this distribution. It also calculates the log probability of the sampled action according to the distribution. The function returns the sampled action and its log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7298b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete_action(policy_nn: PolicyNetwork,\n",
    "                           state: NDArray[np.float64]) -> Tuple[int, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample a discrete action based on the given state and policy network.\n",
    "\n",
    "    This function takes a state and a policy network, and returns a sampled action and its log probability.\n",
    "    The action is sampled from a categorical distribution defined by the output of the policy network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy network that defines the probability distribution of the actions.\n",
    "    state : NDArray[np.float64]\n",
    "        The state based on which an action needs to be sampled.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[int, torch.Tensor]\n",
    "        The sampled action and its log probability.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO...\n",
    "    \n",
    "    # Return the sampled action and its log probability.\n",
    "    return sampled_action, sampled_action_log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48127211",
   "metadata": {},
   "source": [
    "**Task 5.3**: Test the `sample_discrete_action` function on a random state using an untrained policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n.item()\n",
    "\n",
    "# TODO...\n",
    "\n",
    "print(\"state:\", state)\n",
    "print(\"theta:\", theta)\n",
    "print(\"sampled action:\", action)\n",
    "print(\"log probability of the sampled action:\", action_log_probability)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d08b5",
   "metadata": {},
   "source": [
    "#### Implement the sample_one_episode function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98e58f",
   "metadata": {},
   "source": [
    "Remember that in the REINFORCE algorithm, we need to generate a complete trajectory, denoted as $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_1, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_T, \\boldsymbol{s}_T \\}$. This trajectory includes the states, actions, and rewards at each time step, as outlined in the algorithm at the beginning of Part 1.\n",
    "\n",
    "**Task 5.4**: Your task is to implement the `sample_one_episode` function. This function should play one episode using the given policy $\\pi_\\theta$ and return its rollouts. The function should adhere to a fixed horizon $T$, which represents the maximum number of steps in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_one_episode(env: gym.Env,\n",
    "                       policy_nn: PolicyNetwork,\n",
    "                       max_episode_duration: int,\n",
    "                       render: bool = False) -> Tuple[List[NDArray[np.float64]], List[int], List[float], List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Execute one episode within the `env` environment utilizing the policy defined by the `policy_nn` parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to play in.\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy neural network.\n",
    "    max_episode_duration : int\n",
    "        The maximum duration of the episode.\n",
    "    render : bool, optional\n",
    "        Whether to render the environment, by default False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[NDArray[np.float64]], List[int], List[float], List[torch.Tensor]]\n",
    "        The states, actions, rewards, and log probability of action for each time step in the episode.\n",
    "    \"\"\"\n",
    "    state_t, info = env.reset()\n",
    "\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_log_prob_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_states.append(state_t)\n",
    "\n",
    "    for t in range(max_episode_duration):\n",
    "\n",
    "        if render:\n",
    "            env.render_wrapper.render()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode_states, episode_actions, episode_rewards, episode_log_prob_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa617e",
   "metadata": {},
   "source": [
    "**Task 5.5:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40957785",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n.item()\n",
    "\n",
    "# TODO...\n",
    "\n",
    "env.close()\n",
    "env.render_wrapper.make_gif(\"lab6_reinforce_untained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70572ea1",
   "metadata": {},
   "source": [
    "#### Implement a test function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693b241",
   "metadata": {},
   "source": [
    "**Task 5.6**: Implement the `avg_return_on_multiple_episodes` function that test the given policy $\\pi_\\theta$ on `num_episodes` episodes (for fixed horizon $T$) and returns the average reward on the `num_episodes` episodes.\n",
    "\n",
    "The function `avg_return_on_multiple_episodes` is designed to play multiple episodes of a given environment using a specified policy neural network and calculate the average return. It takes as input the environment to play in, the policy neural network to use, the number of episodes to play, the maximum duration of an episode, and an optional parameter to decide whether to render the environment or not. \n",
    "In each episode, it uses the `sample_one_episode` function to play the episode and collect the rewards. The function then returns the average of these cumulated rewards.\n",
    "\n",
    "`avg_return_on_multiple_episodes` will be used for evaluating the performance of a policy over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_return_on_multiple_episodes(env: gym.Env,\n",
    "                                    policy_nn: PolicyNetwork,\n",
    "                                    num_test_episode: int,\n",
    "                                    max_episode_duration: int,\n",
    "                                    render: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Play multiple episodes of the environment and calculate the average return.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to play in.\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy neural network.\n",
    "    num_test_episode : int\n",
    "        The number of episodes to play.\n",
    "    max_episode_duration : int\n",
    "        The maximum duration of an episode.\n",
    "    render : bool, optional\n",
    "        Whether to render the environment, by default False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The average return.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return average_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e627c",
   "metadata": {},
   "source": [
    "**Task 5.7:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n.item()\n",
    "\n",
    "# TODO...\n",
    "\n",
    "print(average_return)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2af8b",
   "metadata": {},
   "source": [
    "#### Implement the train function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca528b57",
   "metadata": {},
   "source": [
    "**Task 5.8**: Implement the `train_reinforce_discrete` function, used to train a policy network using the REINFORCE algorithm in the given environment. This function takes as input the environment, the number of training episodes, the number of tests to perform per episode, the maximum duration of an episode, and the learning rate for the optimizer.\n",
    "\n",
    "The function first initializes a policy network and an Adam optimizer. Then, for each training episode, it generates an episode using the current policy and calculates the return at each time step. It uses this return and the log probability of the action taken at that time step to compute the loss, which is the negative of the product of the return and the log probability. This loss is used to update the policy network parameters using gradient ascent.\n",
    "\n",
    "After each training episode, the function tests the current policy by playing a number of test episodes and calculating the average return. This average return is added to a list for monitoring purposes.\n",
    "\n",
    "The function returns the trained policy network and the list of average returns for each episode. This function encapsulates the main loop of the REINFORCE algorithm, including the policy update step. Please refer back to the algorithm outlined at the start of Part 3 for additional context, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce_discrete(env: gym.Env,\n",
    "                             num_train_episodes: int,\n",
    "                             num_test_per_episode: int,\n",
    "                             max_episode_duration: int,\n",
    "                             learning_rate: float) -> Tuple[PolicyNetwork, List[float]]:\n",
    "    \"\"\"\n",
    "    Train a policy using the REINFORCE algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train in.\n",
    "    num_train_episodes : int\n",
    "        The number of training episodes.\n",
    "    num_test_per_episode : int\n",
    "        The number of tests to perform per episode.\n",
    "    max_episode_duration : int\n",
    "        The maximum length of an episode, by default EPISODE_DURATION.\n",
    "    learning_rate : float\n",
    "        The initial step size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[PolicyNetwork, List[float]]\n",
    "        The final trained policy and the average returns for each episode.\n",
    "    \"\"\"\n",
    "    episode_avg_return_list = []\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n.item()\n",
    "\n",
    "    policy_nn = PolicyNetwork(state_size, action_size).to(device)\n",
    "    optimizer = torch.optim.Adam(policy_nn.parameters(), lr=learning_rate)\n",
    "\n",
    "    for episode_index in tqdm(range(num_train_episodes)):\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        # Test the current policy\n",
    "        test_avg_return = avg_return_on_multiple_episodes(env=env,\n",
    "                                                          policy_nn=policy_nn,\n",
    "                                                          num_test_episode=num_test_per_episode,\n",
    "                                                          max_episode_duration=max_episode_duration,\n",
    "                                                          render=False)\n",
    "\n",
    "        # Monitoring\n",
    "        episode_avg_return_list.append(test_avg_return)\n",
    "\n",
    "    return policy_nn, episode_avg_return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e565d",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59957cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# NUMBER_OF_TRAININGS = 3\n",
    "reinforce_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Train the agent\n",
    "    reinforce_policy_nn, episode_reward_list = train_reinforce_discrete(env=env,\n",
    "                                                                        num_train_episodes=250,\n",
    "                                                                        num_test_per_episode=5,\n",
    "                                                                        max_episode_duration=200,\n",
    "                                                                        learning_rate=0.01)\n",
    "\n",
    "    reinforce_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    reinforce_trains_result_list[1].extend(episode_reward_list)\n",
    "    reinforce_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "reinforce_trains_result_df = pd.DataFrame(np.array(reinforce_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "reinforce_trains_result_df[\"agent\"] = \"REINFORCE\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(reinforce_policy_nn, \"reinforce_policy_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd82c6",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=reinforce_trains_result_df, height=7, aspect=2, alpha=0.5)\n",
    "plt.savefig(\"lab6_reinforce_cartpole_trains_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fc6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", hue=\"agent\", kind=\"line\", data=reinforce_trains_result_df, height=7, aspect=2)\n",
    "plt.savefig(\"lab6_reinforce_cartpole_trains_result_agg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93754e9",
   "metadata": {},
   "source": [
    "#### Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19398652",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n.item()\n",
    "\n",
    "episode_states, episode_actions, episode_rewards, episode_log_prob_actions = sample_one_episode(env, reinforce_policy_nn, 200, render=True)\n",
    "\n",
    "env.close()\n",
    "env.render_wrapper.make_gif(\"lab6_reinforce_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d5cab",
   "metadata": {},
   "source": [
    "**Task 5.9**: decrease the learning rate value (e.g. 0.001), increase the number of episodes per training and retrain the agent. What do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
