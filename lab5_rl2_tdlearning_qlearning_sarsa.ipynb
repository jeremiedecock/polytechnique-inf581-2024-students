{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF581 Lab5: Reinforcement Learning - TD Learning, QLearning and SARSA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2024-students/main/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2024](https://moodle.polytechnique.fr/course/view.php?id=17108) Lab session #5\n",
    "\n",
    "2019-2024 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2024-students/main?filepath=lab5_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2024-students/raw/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab is to introduce some classic concepts used\n",
    "in reinforcement learning: *Temporal Difference Learning* (*TD Learning*), *QLearning* and *SARSA*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: Here we assume that the reward only depends on the state: $r(\\boldsymbol{s}) \\equiv \\mathcal{R}(\\boldsymbol{s}, \\boldsymbol{a}, \\boldsymbol{s}')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires the following Python libraries: *Gymnasium*, NumPy, Pandas, Seaborn and Imageio.\n",
    "\n",
    "### If you use Google Colab\n",
    "\n",
    "Execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_requirements = [\n",
    "    \"gymnasium\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"seaborn\"\n",
    "]\n",
    "import sys, subprocess\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "        \n",
    "if \"google.colab\" in sys.modules:\n",
    "    for i in colab_requirements:\n",
    "        run_subprocess_command(\"pip install \" + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you use MyBinder\n",
    "\n",
    "Required libraries are already installed, you have nothing to do.\n",
    "\n",
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment\n",
    "\n",
    "Uncomment and execute the following cell to install required packages in your local environment (remove only the `#` not the `!`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium imageio numpy pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the FrozenLake toy problem with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided\n",
    "by the Gymnasium framework. Especially, as in Lab 4, we will try to solve the FrozenLake-v1\n",
    "problem (https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "As a reminder, this environment is described [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this environment is *fully observable*, thus here the terms (environment) *state* and (agent) *observation* are equivalent.\n",
    "This is not always the case for example in poker, the agent doesn't know the opponent's cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells contain two functions that can be used to display states and Qtables in the FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtable_display(q_array, title=None, figsize=(4,4), annot=True, fmt=\"0.1f\", linewidths=.5, square=True, cbar=False, cmap=\"Reds\"):\n",
    "    num_actions = q_array.shape[1]\n",
    "\n",
    "    global_figsize = list(figsize)\n",
    "    global_figsize[0] *= num_actions\n",
    "    fig, ax_list = plt.subplots(ncols=num_actions, figsize=global_figsize)   # Sample figsize in inches\n",
    "\n",
    "    for action_index in range(num_actions):\n",
    "        ax = ax_list[action_index]\n",
    "        state_seq = q_array.T[action_index]\n",
    "        states_display(state_seq, title=None, figsize=figsize, annot=True, fmt=\"0.1f\", linewidths=.5, square=True, cbar=False, cmap=\"Reds\", ax=ax)\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_display(state_seq, title=None, figsize=(5,5), annot=True, fmt=\"0.1f\", linewidths=.5, square=True, cbar=False, cmap=\"Reds\", ax=None):\n",
    "    size = int(math.sqrt(len(state_seq)))\n",
    "    state_array = np.array(state_seq)\n",
    "    state_array = state_array.reshape(size, size)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)         # Sample figsize in inches\n",
    "\n",
    "    sns.heatmap(state_array, annot=annot, fmt=fmt, linewidths=linewidths, square=square, cbar=cbar, cmap=cmap, ax=ax)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement the TD Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab4, we have seen Dynamic Programming methods that can be used to solve Markov Decision Problems when the environment is perfectly known to the agent i.e. in cases where the agent knows in advance the transition and the reward functions. This is a strong assumption as in most practical problems, these functions are not known in advance. In this lab, we will see how to make agents that can solve Markov Decision Problems without prior knowledge on the environment i.e. agents that learn the dynamics of their environment by exploring it and use this knowledge to find an optimal policy.\n",
    "\n",
    "We will start with the *TD Learning* (*Temporal Difference Learning*) algorithm that can be used to **evaluate** any **given policy** (i.e. compute the *value function* or *V Table* of the environment following the given policy that is to say the expected value of any state when the agent follow the provided policy).\n",
    "However, the main ideas of this algorithm can in turn be used within optimal control methods like e.g. https://en.wikipedia.org/wiki/TD-Gammon.\n",
    "Exercises 2 to 4 also reuse the main concepts of TD Learning to calculate an optimal policy. \n",
    "\n",
    "The algorithm is recalled below.\n",
    "\n",
    "---\n",
    "TD Learning\n",
    "---\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ the policy $\\pi$ to be evaluated<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br><br>\n",
    "\n",
    "Initialize arbitrarily $V(\\boldsymbol{s}) ~~~ \\forall \\boldsymbol{s} \\in \\mathcal{S}$<br>\n",
    "$V(\\boldsymbol{s}_F) \\leftarrow 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (initialize finale states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $S \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $A \\leftarrow \\pi(S)$<br>\n",
    "\t\t$\\quad\\quad$ $S', R \\leftarrow \\text{env.step}(A)$<br>\n",
    "\t\t$\\quad\\quad$ $V(S) \\leftarrow V(S) + \\alpha \\left[ \\underbrace{\\overbrace{R + \\gamma ~ V(S')}^{\\text{Target for } V(S)} ~ - ~ V(S)}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $S \\leftarrow S'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $S$ is final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: in the following cell, `policy` is a list of actions (one per state c.f. two cells bellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function_history = []\n",
    "alpha_history = []\n",
    "\n",
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "def td_learning(policy, environment, alpha=0.1, alpha_factor=0.995, gamma=0.95, num_episodes=1000, display=False):\n",
    "    num_states = environment.observation_space.n\n",
    "    v_array = np.zeros(num_states)   # Initial value function\n",
    "\n",
    "    for episode_index in range(num_episodes):\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            states_display(v_array, title=\"Value function (ep. {})\".format(episode_index), cbar=True, cmap=\"Reds\")\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "        value_function_history.append(v_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "                \n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO...\n",
    "    \n",
    "    return v_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the value function `v_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = [0, 3, 3, 3,\n",
    "          0, 0, 0, 0,\n",
    "          3, 1, 0, 0,\n",
    "          0, 2, 1, 0]\n",
    "\n",
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "v_array = td_learning(policy, environment, display=False)\n",
    "\n",
    "environment.close()\n",
    "\n",
    "states_display(v_array, title=\"Value function\", cbar=True, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_hist = pd.DataFrame(value_function_history)\n",
    "df_v_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of `v_array` (the estimated value of each state) over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_hist.plot(figsize=(14,8))\n",
    "plt.title(\"V(s) w.r.t iteration\")\n",
    "plt.ylabel(\"V(s)\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(alpha_history)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: The learning rate $\\alpha$ in TD-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, set `alpha_factor` to 1 and check the algorithm with different values between 0 and 1.\n",
    "\n",
    "What do you observe ?\n",
    "What is the role of `alpha_factor` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement the Greedy and Epsilon Greedy policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exercise 1, TD-Learning has been used to **estimate the value** of a **given policy**.\n",
    "In the following exercises, we will now see how to **find the optimal** (or a nearly optimal) policy.\n",
    "For that, we will use two algorithms (SARSA and QLearning) that estimate a *QTable* (or *action-value function*) instead of a VTable (or value function).\n",
    "This QTable gives the expected reward when the agent plays a given action $A$ from any given state $S$ and then follow a given *exploration policy* to choose the following actions until a final state is reached (and this exploration policy uses the QTable to choose actions to play).\n",
    "While the agent explore the environment, it update it's QTable using an *update policy*.\n",
    "\n",
    "The purpose of this third exercise is to implement the *greedy* and the $\\epsilon$-*greedy* policies that agents will used to explore the environment and update their QTable:\n",
    "\n",
    "$\\displaystyle \\pi^{Q^{\\pi}}(S) := \\text{greedy}(S, Q^{\\pi}) = \\arg\\max_{A \\in \\mathcal{A}} Q^{\\pi}(S, A)$\n",
    "\n",
    "\n",
    "$\\pi^{Q^{\\pi},\\epsilon}(S) := \\epsilon\\text{-greedy}(S, Q^{\\pi}) = $\n",
    "randomly choose between $\\underbrace{\\text{greedy}(S, Q^{\\pi})}_{\\text{with probability } 1 - \\epsilon}$\n",
    "and $~~ \\underbrace{\\text{a random action}}_{\\text{with probability } \\epsilon}$    i.e. $\\epsilon \\in (0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, q_array):\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, q_array, epsilon):\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implement the SARSA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal policy (or a nearly optimal policy) for the FrozenLake-v1 problem, we will first use the SARSA algorithm.\n",
    "It is based on the online update of the so-called *QTable* (or *Q-function* or *action value function*) for the current policy defined as:\n",
    "$$\n",
    "Q^{\\pi}(s, a) = \\mathbb{E}^{\\pi} \\left[ \\sum_{t=0}^{H} \\gamma^t r(s_t, a_t) | s=s_0, a=a_0 \\right] ,\n",
    "$$\n",
    "where $\\gamma \\in [0, 1]$ is the discount factor, and $H$ the horizon of the episode.\n",
    "\n",
    "The SARSA algorithm updates a tabular estimate of the Q-function using the following update rule:\n",
    "$$\n",
    "Q^{\\pi}_{t+1} (s_t , a_t) \\leftarrow Q^{\\pi}_t(s_t, a_t) + \\alpha \\left( r_t + \\gamma Q^{\\pi}_t(s_{t+1}, a_{t+1}) - Q^{\\pi}_t(s_t, a_t) \\right) ,\n",
    "$$\n",
    "where $\\alpha \\in (0, 1]$ is the learning rate, and $r_t$ is the reward received by the agent at time step $t$.\n",
    "Most of the time, the SARSA algorithm is implemented with an $\\epsilon$-greedy exploration strategy.\n",
    "This strategy consists in selecting the best action learned so far with probability $(1 - \\epsilon)$ and to select a random\n",
    "1action with probability $\\epsilon$.\n",
    "\n",
    "**Tasks**: Implement the SARSA algorithm with $\\epsilon$-greedy exploration (start with $\\epsilon = 0.5$).\n",
    "\n",
    "---\n",
    "SARSA\n",
    "---\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "Initialize arbitrarily $Q(\\boldsymbol{s}, \\boldsymbol{a}) ~~~ \\forall \\boldsymbol{s} \\in \\mathcal{S}, \\boldsymbol{a} \\in \\mathcal{A}(\\boldsymbol{s})$<br>\n",
    "$Q(\\boldsymbol{s}_F, \\cdot) = 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (initialize finale states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $S \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ $A \\leftarrow \\epsilon\\text{-greedy}(S, Q)$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $R, S' \\leftarrow \\text{env.step}(A)$<br>\n",
    "\t\t$\\quad\\quad$ $A' \\leftarrow \\epsilon\\text{-greedy}(S', Q)$<br>\n",
    "\t\t$\\quad\\quad$ $Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ \\underbrace{R + \\gamma ~ Q(S',A') ~ - ~ Q(S,A)}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $S \\leftarrow S'$<br>\n",
    "\t\t$\\quad\\quad$ $A \\leftarrow A'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $S$ is final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_array_history = []\n",
    "alpha_history = []\n",
    "\n",
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "def sarsa(environment, alpha=0.1, alpha_factor=0.9995, gamma=0.99, epsilon=0.5, num_episodes=10000, display=False):\n",
    "    num_states = environment.observation_space.n\n",
    "    num_actions = environment.action_space.n\n",
    "    q_array = np.zeros([num_states, num_actions])   # Initial Q table\n",
    "\n",
    "    for episode_index in range(num_episodes):\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            qtable_display(q_array, title=\"Q table\", cbar=True)\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "        q_array_history.append(q_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Update alpha\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "        \n",
    "        # TODO...\n",
    "\n",
    "    return q_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the action-value function `q_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "q_array = sarsa(environment, display=False)\n",
    "\n",
    "environment.close()\n",
    "\n",
    "qtable_display(q_array, title=\"Q Table\", cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_array_history = np.array(q_array_history)\n",
    "df_q_hist_list = []\n",
    "\n",
    "for action_index in range(q_array_history.shape[2]):\n",
    "    df_q_hist_list.append(pd.DataFrame(q_array_history[:, :, action_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of `q_array` over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action_index, df_q_hist in enumerate(df_q_hist_list):\n",
    "    df_q_hist.plot(figsize=(14,8))\n",
    "    plt.title(\"Q(s,{}) w.r.t iteration\".format(action_index))\n",
    "    plt.ylabel(\"Q(s,{})\".format(action_index))\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(alpha_history)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Policy with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a measure of performance, count the number of successful trials on 1000 episodes.\n",
    "\n",
    "**Note**: Gymnasium considers the task is solved if you reach 76\\% of success over the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "reward_list = []\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "    done = False\n",
    "    #t = 0\n",
    "\n",
    "    while not done:\n",
    "        #action = epsilon_greedy_policy(state, q_array, epsilon)\n",
    "        action = greedy_policy(state, q_array)\n",
    "        state, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "reward_df = pd.DataFrame(reward_list)\n",
    "\n",
    "print('Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):', np.average(reward_df))\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Implement the QLearning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another reinforcement learning algorithm is the so called Q-Learning algorithm.\n",
    "The fundamental difference with SARSA is that it is an off-policy algorithm.\n",
    "This means that it doesn't estimate the Q-function of its current policy but it estimates the value of another policy which is the optimal one.\n",
    "To do so, it uses the following update rule:\n",
    "$$\n",
    "Q_{t+1}(s_t, a_t) \\leftarrow Q_t(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_b Q_t(s_{t+1}, b) - Q_t(s_t, a_t) \\right) .\n",
    "$$\n",
    "\n",
    "**Task**: in this exercise, you will replace the SARSA update rule by the Q-learning one and analyze the\n",
    "differences in performances.\n",
    "\n",
    "---\n",
    "QLearning\n",
    "---\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "Initialize arbitrarily $Q(\\boldsymbol{s}, \\boldsymbol{a}) ~~~ \\forall \\boldsymbol{s} \\in \\mathcal{S}, \\boldsymbol{a} \\in \\mathcal{A}(\\boldsymbol{s})$<br>\n",
    "$Q(\\boldsymbol{s}_F, \\cdot) = 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (initialize finale states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $S \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $A \\leftarrow \\epsilon\\text{-greedy}(S, Q)$<br>\n",
    "\t\t$\\quad\\quad$ $R, S' \\leftarrow \\text{env.step}(A)$<br>\n",
    "\t\t$\\quad\\quad$ $Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ \\underbrace{R + \\gamma ~ \\max_{\\boldsymbol{a}} Q(S', \\boldsymbol{a}) ~ - ~ Q(S,A)}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $S \\leftarrow S'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $S$ is final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_array_history = []\n",
    "alpha_history = []\n",
    "\n",
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "def q_learning(environment, alpha=0.1, alpha_factor=0.9995, gamma=0.99, epsilon=0.5, num_episodes=10000, display=False):\n",
    "    num_states = environment.observation_space.n\n",
    "    num_actions = environment.action_space.n\n",
    "    q_array = np.zeros([num_states, num_actions])   # Initial Q table\n",
    "\n",
    "    for episode_index in range(num_episodes):\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            qtable_display(q_array, title=\"Q table\", cbar=True)\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "        q_array_history.append(q_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Update alpha\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO...\n",
    "    \n",
    "    return q_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the action-value function `q_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "q_array = q_learning(environment, display=False)\n",
    "\n",
    "environment.close()\n",
    "\n",
    "qtable_display(q_array, title=\"Q Table\", cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_array_history = np.array(q_array_history)\n",
    "df_q_hist_list = []\n",
    "\n",
    "for action_index in range(q_array_history.shape[2]):\n",
    "    df_q_hist_list.append(pd.DataFrame(q_array_history[:, :, action_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of `q_array` over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action_index, df_q_hist in enumerate(df_q_hist_list):\n",
    "    df_q_hist.plot(figsize=(14,8))\n",
    "    plt.title(\"Q(s,{}) w.r.t iteration\".format(action_index))\n",
    "    plt.ylabel(\"Q(s,{})\".format(action_index))\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(alpha_history)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Policy with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a measure of performance, count the number of successful trials on 1000 episodes.\n",
    "\n",
    "**Note**: Gymnasium considers the task is solved if you reach 76\\% of success over the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "reward_list = []\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "    done = False\n",
    "    #t = 0\n",
    "\n",
    "    while not done:\n",
    "        #action = epsilon_greedy_policy(state, q_array, epsilon)\n",
    "        action = greedy_policy(state, q_array)\n",
    "        state, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "reward_df = pd.DataFrame(reward_list)\n",
    "\n",
    "print('Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):', np.average(reward_df))\n",
    "\n",
    "environment.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
