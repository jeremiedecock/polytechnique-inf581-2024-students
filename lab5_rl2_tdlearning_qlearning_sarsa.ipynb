{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee56f65",
   "metadata": {},
   "source": [
    "# INF581 Lab5: Reinforcement Learning - TD Learning, QLearning and SARSA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2024-students/main/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2024](https://moodle.polytechnique.fr/course/view.php?id=17108) Lab session #5\n",
    "\n",
    "2019-2024 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222d24e",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2024-students/main?filepath=lab5_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2024-students/blob/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2024-students/raw/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4d84d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab is to introduce some classic concepts used\n",
    "in reinforcement learning: *Temporal Difference Learning* (*TD Learning*), *QLearning* and *SARSA*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c2e93",
   "metadata": {},
   "source": [
    "**Notice**: Here we assume that the reward only depends on the state: $r(\\boldsymbol{s}) \\equiv \\mathcal{R}(\\boldsymbol{s}, \\boldsymbol{a}, \\boldsymbol{s}')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43e4b8",
   "metadata": {},
   "source": [
    "## Name your work\n",
    "\n",
    "Replace the values in the following dictionary `info`. Your Email must match your class email address. Your Alias will be shown on the public leaderboard (to identify yourself). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "        'Email' : 'firstname.lastname@polytechnique.edu',\n",
    "        'Alias' : 'Anonymous', # (change this in case you want to identify yourself on the leaderboard)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f485fa11",
   "metadata": {},
   "source": [
    "## Python requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5cb94a",
   "metadata": {},
   "source": [
    "This notebook requires the following Python libraries: *Gymnasium*, NumPy, Pandas, Seaborn.\n",
    "\n",
    "### If you use Google Colab\n",
    "\n",
    "Execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_requirements = [\n",
    "    \"gymnasium\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"seaborn\"\n",
    "]\n",
    "import sys, subprocess\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "        \n",
    "if \"google.colab\" in sys.modules:\n",
    "    for i in colab_requirements:\n",
    "        run_subprocess_command(\"pip install \" + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a4fda",
   "metadata": {},
   "source": [
    "### If you use MyBinder\n",
    "\n",
    "Required libraries are already installed, you have nothing to do.\n",
    "\n",
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment\n",
    "\n",
    "Uncomment and execute the following cell to install required packages in your local environment (remove only the `#` not the `!`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium numpy pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35184258",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import List, Optional, Tuple, Union, Callable, Dict, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e19825",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16595812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20423459",
   "metadata": {},
   "source": [
    "## Setup the FrozenLake toy problem with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00db6d0",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided\n",
    "by the Gymnasium framework. Especially, as in Lab 4, we will try to solve the FrozenLake-v1\n",
    "problem (https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "As a reminder, this environment is described [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\">\n",
    "\n",
    "The action indices are outlined below:\n",
    "\n",
    "| Action Index | Action     |\n",
    "|--------------|------------|\n",
    "| 0            | Move Left  |\n",
    "| 1            | Move Down  |\n",
    "| 2            | Move Right |\n",
    "| 3            | Move Up    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11664fd",
   "metadata": {},
   "source": [
    "The following dictionary may be used to understand actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_labels = {\n",
    "    0: \"Move Left\",\n",
    "    1: \"Move Down\",\n",
    "    2: \"Move Right\",\n",
    "    3: \"Move Up\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3d75f1",
   "metadata": {},
   "source": [
    "**Notice**: this environment is *fully observable*, thus here the terms (environment) *state* and (agent) *observation* are equivalent.\n",
    "This is not always the case for example in poker, the agent doesn't know the opponent's cards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f2974a",
   "metadata": {},
   "source": [
    "### Display functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf586f7e",
   "metadata": {},
   "source": [
    "The next cells contain two functions that can be used to display Q-tables in the FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Q-table as a set of heatmaps, one for each action\n",
    "def qtable_display(\n",
    "    q_array: np.ndarray, \n",
    "    title: Optional[str] = None, \n",
    "    figsize: Tuple[int, int] = (4, 4), \n",
    "    annot: bool = True, \n",
    "    fmt: str = \"0.1f\", \n",
    "    linewidths: float = .5, \n",
    "    square: bool = True, \n",
    "    cbar: bool = False, \n",
    "    cmap: str = \"Reds\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Display a Q-table as a set of heatmaps, one for each action.\n",
    "\n",
    "    For the frozen lake environment, there are 16 states and 4 actions thus this function will display 4 heatmaps, one for each action.\n",
    "    Each heatmap will display the Q-values for each state when performing the action indexed by the heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q_array : np.ndarray\n",
    "        The Q-table to display. Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    title : str, optional\n",
    "        The title of the plot, by default None\n",
    "    figsize : tuple, optional\n",
    "        The size of the figure (in inches), by default (4, 4)\n",
    "    annot : bool, optional\n",
    "        If True, write the data value in each cell, by default True\n",
    "    fmt : str, optional\n",
    "        The string formatting code to use when adding annotations, by default \"0.1f\" that will display a single decimal\n",
    "    linewidths : float, optional\n",
    "        The width of the lines that will divide each cell, by default .5\n",
    "    square : bool, optional\n",
    "        Whether to set the Axes aspect to \"equal\" so each cell is square-shaped, by default True\n",
    "    cbar : bool, optional\n",
    "        Whether to draw a colorbar, by default False\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space, by default \"Reds\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the number of actions from the shape of the Q-table\n",
    "    num_actions = q_array.shape[1]\n",
    "\n",
    "    # Adjust the figure size (in inches) based on the number of actions\n",
    "    global_figsize = list(figsize)\n",
    "    global_figsize[0] *= num_actions\n",
    "\n",
    "    # Create a subplot for each action\n",
    "    fig, ax_list = plt.subplots(ncols=num_actions, figsize=global_figsize)\n",
    "\n",
    "    # For each action, display the Q-values for all states as a heatmap\n",
    "    for action_index in range(num_actions):\n",
    "        ax = ax_list[action_index]\n",
    "\n",
    "        # Retrieve the Q-values for each state when performing the action indexed by \"action_index\".\n",
    "        # This forms a 1D array, state_vec, where state_vec[i] = Q(i, action_index).\n",
    "        state_vec = q_array[:,action_index]\n",
    "\n",
    "        # Display the Q-values for each state when performing the action indexed by \"action_index\"\n",
    "        # i.e. display Q(., action_index)\n",
    "        states_display(\n",
    "            state_vec,\n",
    "            title=r\"$Q(\\cdot,a_{})$\".format(action_index),\n",
    "            #title=r\"$Q(\\cdot,a_{})$ {}\".format(action_index, action_labels[action_index]),\n",
    "            figsize=figsize, \n",
    "            annot=annot, \n",
    "            fmt=fmt, \n",
    "            linewidths=linewidths, \n",
    "            square=square, \n",
    "            cbar=cbar, \n",
    "            cmap=cmap, \n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "    # Set the title for the entire figure\n",
    "    plt.suptitle(title)\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c03b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_display(\n",
    "    state_seq: Sequence[float], \n",
    "    title: Optional[str] = None, \n",
    "    figsize: Tuple[int, int] = (5, 5), \n",
    "    annot: bool = True, \n",
    "    fmt: str = \"0.1f\", \n",
    "    linewidths: float = .5, \n",
    "    square: bool = True, \n",
    "    cbar: bool = False, \n",
    "    cmap: str = \"Reds\", \n",
    "    ax: Optional[matplotlib.axes.Axes] = None\n",
    ") -> Optional[matplotlib.axes.Axes]:\n",
    "    \"\"\"\n",
    "    Display the expected values of all states as a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_seq : Sequence[float]\n",
    "        The sequence of expected values to display. This can be a list, a 1D array, etc.\n",
    "        Each element is the estimation of the expected value of the corresponding state.\n",
    "        For example, state_seq[0] is the estimation of the expected value of the first state.\n",
    "        There are 16 elements in this sequence for the frozenlake environment, i.e., one per state of the environment.\n",
    "    title : str, optional\n",
    "        The title of the plot, by default None\n",
    "    figsize : tuple, optional\n",
    "        The size of the figure (in inches), by default (5, 5)\n",
    "    annot : bool, optional\n",
    "        If True, write the data value in each cell, by default True\n",
    "    fmt : str, optional\n",
    "        The string formatting code to use when adding annotations, by default \"0.1f\"\n",
    "    linewidths : float, optional\n",
    "        The width of the lines that will divide each cell, by default .5\n",
    "    square : bool, optional\n",
    "        Whether to set the Axes aspect to \"equal\" so each cell is square-shaped, by default True\n",
    "    cbar : bool, optional\n",
    "        Whether to draw a colorbar, by default False\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space, by default \"Reds\"\n",
    "    ax : matplotlib.axes.Axes, optional\n",
    "        The axes object to draw the heatmap on, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.axes.Axes, optional\n",
    "        The axes object with the heatmap if one was provided, otherwise None.\n",
    "    \"\"\"\n",
    "    # Calculate the size of the state array\n",
    "    size = int(math.sqrt(len(state_seq)))\n",
    "    # Convert the state sequence to a numpy array (if it isn't already one)\n",
    "    state_array = np.array(state_seq)\n",
    "    # Reshape the state array into a square matrix\n",
    "    # (we assume here that the frozen lake environment is used,\n",
    "    # thus the state space can be visualized as a square grid)\n",
    "    state_array = state_array.reshape(size, size)\n",
    "\n",
    "    # If no axes object is provided, create a new figure and axes\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Create a heatmap of the state array on the axes\n",
    "    ax = sns.heatmap(\n",
    "        state_array, \n",
    "        annot=annot, \n",
    "        fmt=fmt, \n",
    "        linewidths=linewidths, \n",
    "        square=square, \n",
    "        cbar=cbar, \n",
    "        cmap=cmap,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # If a title is provided, set the title of the plot\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # If no axes object was provided, display the plot\n",
    "    # Otherwise, return the axes object with the heatmap\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113e59d",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement the TD Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82beae72",
   "metadata": {},
   "source": [
    "In Lab4, we explored Dynamic Programming methods that can be used to solve Markov Decision Problems when the environment is perfectly known to the agent, i.e., in cases where the agent knows the transition and reward functions in advance. However, this is a strong assumption, as in most practical problems, these functions are not known beforehand. In this lab, we will learn how to create agents that can solve Markov Decision Problems without prior knowledge of the environment. These agents learn the dynamics of their environment by exploring it and use this knowledge to find an optimal policy.\n",
    "\n",
    "We will start with the *TD Learning* (*Temporal Difference Learning*) algorithm, which can be used to **evaluate** any **given policy** (i.e., compute the *value function* or *V Table* of the environment following the given policy, which is the expected value of any state when the agent follows the provided policy).\n",
    "However, the main ideas of this algorithm can also be used within optimal control methods, like e.g., [TD-Gammon](https://en.wikipedia.org/wiki/TD-Gammon).\n",
    "Exercises 2 to 4 also reuse the main concepts of TD Learning to calculate an optimal policy. \n",
    "\n",
    "The algorithm is outlined below.\n",
    "\n",
    "---\n",
    "TD Learning\n",
    "---\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ the policy $\\pi$ to be evaluated<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br><br>\n",
    "\n",
    "Initialize arbitrarily $V(\\boldsymbol{s}) ~~~ \\forall \\boldsymbol{s} \\in \\mathcal{S}$<br>\n",
    "$V(\\boldsymbol{s}_F) \\leftarrow 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (initialize finale states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $S \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $A \\leftarrow \\pi(S)$<br>\n",
    "\t\t$\\quad\\quad$ $S', R \\leftarrow \\text{env.step}(A)$<br>\n",
    "\t\t$\\quad\\quad$ $V(S) \\leftarrow V(S) + \\alpha \\left[ \\underbrace{\\overbrace{R + \\gamma ~ V(S')}^{\\text{Target for } V(S)} ~ - ~ V(S)}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $S \\leftarrow S'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $S$ is final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ccd50c",
   "metadata": {},
   "source": [
    "**Notice**: in the following cell, `policy` is a list of actions (one per state c.f. two cells bellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "# Initialize the history of the value function and learning rate\n",
    "value_function_history_ex1 = []\n",
    "alpha_history_ex1 = []\n",
    "\n",
    "def td_learning(\n",
    "    policy: Sequence[float], \n",
    "    environment: gym.Env, \n",
    "    alpha: float = 0.1, \n",
    "    alpha_factor: float = 0.995, \n",
    "    gamma: float = 0.95, \n",
    "    num_episodes: int = 1000, \n",
    "    display: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform Temporal Difference learning on a given policy and environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : Sequence[float]\n",
    "        The policy to be learned, represented as a sequence mapping states (the index of the sequence) to actions (the value of the sequence for this index).\n",
    "        For example, policy[0] is the action to take in state 0.\n",
    "    environment : gym.Env\n",
    "        The environment in which the agent operates.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor by which the learning rate alpha decreases each episode, by default 0.995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.95\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 1000\n",
    "    display : bool, optional\n",
    "        Whether to display the value function (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The learned value function. This is a 1D ndarray with a shape of (16,) as there are 16 states in the frozenlake environment.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of states in the environment\n",
    "    num_states = environment.observation_space.n\n",
    "    # Initialize the value function to zeros\n",
    "    v_array = np.zeros(num_states)\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in range(num_episodes):\n",
    "        # Display the value function every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            states_display(\n",
    "                v_array, \n",
    "                title=f\"Value function (ep. {episode_index})\", \n",
    "                cbar=True, \n",
    "                cmap=\"Reds\"\n",
    "            )\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "\n",
    "        # Save the current value function and learning rate\n",
    "        value_function_history_ex1.append(v_array.copy())\n",
    "        alpha_history_ex1.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    # Return the learned value function\n",
    "    return v_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5576dd2",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the value function `v_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35887320",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the policy to evaluate\n",
    "policy = [0, 3, 3, 3,\n",
    "          0, 0, 0, 0,\n",
    "          3, 1, 0, 0,\n",
    "          0, 2, 1, 0]\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "# Initialize the history of the value function and learning rate\n",
    "value_function_history_ex1.clear()\n",
    "alpha_history_ex1.clear()\n",
    "\n",
    "# Apply Temporal Difference (TD) Learning to calculate the value function for the policy defined earlier, within the context of the FrozenLake environment.\n",
    "v_array = td_learning(policy, environment, display=False)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988de387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the learned value function\n",
    "states_display(v_array, title=\"Value function\", cbar=True, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe38fc",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31df0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evolution of the value function during the learning process\n",
    "df_v_hist_ex1 = pd.DataFrame(value_function_history_ex1)\n",
    "df_v_hist_ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff79ea",
   "metadata": {},
   "source": [
    "Evolution of `v_array` (the estimated expected value of each state) over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the estimated expected value for each state during the learning process\n",
    "df_v_hist_ex1.plot(figsize=(14,8))\n",
    "plt.title(r\"$V^{\\pi}(\\cdot)$ w.r.t iteration\")\n",
    "plt.ylabel(r\"$V^{\\pi}(\\cdot)$\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the alpha parameter during the learning process\n",
    "plt.loglog(alpha_history_ex1)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc86233",
   "metadata": {},
   "source": [
    "## Exercise 2: The learning rate $\\alpha$ in TD-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2246f",
   "metadata": {},
   "source": [
    "In the previous exercise, set `alpha_factor` to 1 and check the algorithm with different values between 0 and 1.\n",
    "\n",
    "What do you observe ?\n",
    "What is the role of `alpha_factor` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01a364",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement the Greedy and Epsilon Greedy policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea39d51",
   "metadata": {},
   "source": [
    "In exercise 1, TD-Learning has been used to **estimate the value** of a **given policy**.\n",
    "In the following exercises, we will now see how to **find the optimal** (or a nearly optimal) policy.\n",
    "For that, we will use two algorithms (SARSA and QLearning) that estimate a *QTable* (or *action-value function*) instead of a VTable (or value function).\n",
    "This QTable gives the expected reward when the agent plays a given action $A$ from any given state $S$ and then follow a given *exploration policy* to choose the following actions until a final state is reached (and this exploration policy uses the QTable to choose actions to play).\n",
    "While the agent explore the environment, it update it's QTable using an *update policy*.\n",
    "\n",
    "The purpose of this third exercise is to implement the *greedy* and the $\\epsilon$-*greedy* policies that agents will used to explore the environment and update their QTable:\n",
    "\n",
    "$\\displaystyle \\pi^{Q^{\\pi}}(S) := \\text{greedy}(S, Q^{\\pi}) = \\arg\\max_{A \\in \\mathcal{A}} Q^{\\pi}(S, A)$\n",
    "\n",
    "\n",
    "$\\pi^{Q^{\\pi},\\epsilon}(S) := \\epsilon\\text{-greedy}(S, Q^{\\pi}) = $\n",
    "randomly choose between $\\underbrace{\\text{greedy}(S, Q^{\\pi})}_{\\text{with probability } 1 - \\epsilon}$\n",
    "and $~~ \\underbrace{\\text{a random action}}_{\\text{with probability } \\epsilon}$    i.e. $\\epsilon \\in (0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe538c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state: int, q_array: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Determine the action that maximizes the Q-value for a given state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The current state.\n",
    "    q_array : np.ndarray\n",
    "        The Q-table.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The action that maximizes the Q-value for the given state.\n",
    "    \"\"\"\n",
    "    # TODO...\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state: int, q_array: np.ndarray, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "    Determine the action to take based on an epsilon-greedy policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The current state.\n",
    "    q_array : np.ndarray\n",
    "        The Q-table.\n",
    "    epsilon : float\n",
    "        The probability of choosing a random action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The action to take.\n",
    "    \"\"\"\n",
    "    # TODO...\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba70c9",
   "metadata": {},
   "source": [
    "## Exercise 4: Implement the SARSA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c04bbc",
   "metadata": {},
   "source": [
    "To find the optimal policy (or a nearly optimal policy) for the FrozenLake-v1 problem, we will first use the SARSA algorithm.\n",
    "It is based on the online update of the so-called *QTable* (or *Q-function* or *action value function*) for the current policy defined as:\n",
    "$$\n",
    "Q^{\\pi}(s, a) = \\mathbb{E}^{\\pi} \\left[ \\sum_{t=0}^{H} \\gamma^t r(s_t, a_t) | s=s_0, a=a_0 \\right] ,\n",
    "$$\n",
    "where $\\gamma \\in [0, 1]$ is the discount factor, and $H$ the horizon of the episode.\n",
    "\n",
    "The SARSA algorithm updates a tabular estimate of the Q-function using the following update rule:\n",
    "$$\n",
    "Q^{\\pi}_{t+1} (s_t , a_t) \\leftarrow Q^{\\pi}_t(s_t, a_t) + \\alpha \\left( r_t + \\gamma Q^{\\pi}_t(s_{t+1}, a_{t+1}) - Q^{\\pi}_t(s_t, a_t) \\right) ,\n",
    "$$\n",
    "where $\\alpha \\in (0, 1]$ is the learning rate, and $r_t$ is the reward received by the agent at time step $t$.\n",
    "Most of the time, the SARSA algorithm is implemented with an $\\epsilon$-greedy exploration strategy.\n",
    "This strategy consists in selecting the best action learned so far with probability $(1 - \\epsilon)$ and to select a random\n",
    "1action with probability $\\epsilon$.\n",
    "\n",
    "**Tasks**: Implement the SARSA algorithm with $\\epsilon$-greedy exploration (start with $\\epsilon = 0.5$).\n",
    "\n",
    "---\n",
    "SARSA\n",
    "---\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "Initialize arbitrarily $Q(\\boldsymbol{s}, \\boldsymbol{a}) ~~~ \\forall \\boldsymbol{s} \\in \\mathcal{S}, \\boldsymbol{a} \\in \\mathcal{A}(\\boldsymbol{s})$<br>\n",
    "$Q(\\boldsymbol{s}_F, \\cdot) = 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (initialize finale states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $S \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ $A \\leftarrow \\epsilon\\text{-greedy}(S, Q)$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $R, S' \\leftarrow \\text{env.step}(A)$<br>\n",
    "\t\t$\\quad\\quad$ $A' \\leftarrow \\epsilon\\text{-greedy}(S', Q)$<br>\n",
    "\t\t$\\quad\\quad$ $Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ \\underbrace{R + \\gamma ~ Q(S',A') ~ - ~ Q(S,A)}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $S \\leftarrow S'$<br>\n",
    "\t\t$\\quad\\quad$ $A \\leftarrow A'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $S$ is final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cd866",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "# Initialize the history of the Q-table and learning rate\n",
    "q_array_history_ex4 = []\n",
    "alpha_history_ex4 = []\n",
    "\n",
    "def sarsa(\n",
    "    environment: gym.Env, \n",
    "    alpha: float = 0.1, \n",
    "    alpha_factor: float = 0.9995, \n",
    "    gamma: float = 0.99, \n",
    "    epsilon: float = 0.5, \n",
    "    num_episodes: int = 10000, \n",
    "    display: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform SARSA learning on a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    environment : gym.Env\n",
    "        The environment to learn in.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor to decrease alpha by each episode, by default 0.9995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.99\n",
    "    epsilon : float, optional\n",
    "        The probability of choosing a random action, by default 0.5\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 10000\n",
    "    display : bool, optional\n",
    "        Whether to display the Q-table (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The learned Q-table.\n",
    "        Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    \"\"\"\n",
    "    # Get the number of states and actions in the environment\n",
    "    num_states = environment.observation_space.n\n",
    "    num_actions = environment.action_space.n\n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    q_array = np.zeros([num_states, num_actions])\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in range(num_episodes):\n",
    "        # Display the Q-table every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            qtable_display(q_array, title=\"Q table\")\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "\n",
    "        # Save the current Q-table and learning rate\n",
    "        q_array_history_ex4.append(q_array.copy())\n",
    "        alpha_history_ex4.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    # Return the learned Q-table\n",
    "    return q_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937482ac",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the action-value function `q_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e228afc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "# Initialize the history of the action-value function and learning rate\n",
    "q_array_history_ex4.clear()\n",
    "alpha_history_ex4.clear()\n",
    "\n",
    "# Apply SARSA to calculate the Q-table for the FrozenLake environment\n",
    "q_array = sarsa(environment, display=False)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a32a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learned Q-table\n",
    "qtable_display(q_array, title=\"Q Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf83603",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Pandas dataframe containing the evolution of the Q-table during the learning process\n",
    "q_array_history_array_ex4 = np.array(q_array_history_ex4)\n",
    "df_q_hist_list_ex4 = []\n",
    "\n",
    "for action_index in range(q_array_history_array_ex4.shape[2]):\n",
    "    df_q_hist_list_ex4.append(pd.DataFrame(q_array_history_array_ex4[:, :, action_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abef54f",
   "metadata": {},
   "source": [
    "Evolution of `q_array` over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892cf72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the estimated expected value for each action and state during the learning process\n",
    "for action_index, df_q_hist in enumerate(df_q_hist_list_ex4):\n",
    "    df_q_hist.plot(figsize=(14,8))\n",
    "    plt.title(r'$Q(\\cdot,a_{})$ w.r.t iteration with $a_{}$ := \"{}\"'.format(action_index, action_index, action_labels[action_index]))\n",
    "    plt.ylabel(r\"$Q(\\cdot,a_{})$\".format(action_index))\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80762ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the alpha parameter during the learning process\n",
    "plt.loglog(alpha_history_ex4)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84eaea5",
   "metadata": {},
   "source": [
    "### Evaluate Policy with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f3b15",
   "metadata": {},
   "source": [
    "As a measure of performance, count the number of successful trials on 1000 episodes.\n",
    "\n",
    "**Note**: Gymnasium considers the task is solved if you reach 76\\% of success over the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "reward_list = []\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "    done = False\n",
    "    #t = 0\n",
    "\n",
    "    while not done:\n",
    "        #action = epsilon_greedy_policy(state, q_array, epsilon)\n",
    "        action = greedy_policy(state, q_array)\n",
    "        state, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "reward_df = pd.DataFrame(reward_list)\n",
    "\n",
    "print('Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):', np.average(reward_df))\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66f28c",
   "metadata": {},
   "source": [
    "## Exercise 5: Implement the QLearning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee21037",
   "metadata": {},
   "source": [
    "Another reinforcement learning algorithm is the so called Q-Learning algorithm.\n",
    "The fundamental difference with SARSA is that it is an off-policy algorithm.\n",
    "This means that it doesn't estimate the Q-function of its current policy but it estimates the value of another policy which is the optimal one.\n",
    "To do so, it uses the following update rule:\n",
    "$$\n",
    "Q_{t+1}(s_t, a_t) \\leftarrow Q_t(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_b Q_t(s_{t+1}, b) - Q_t(s_t, a_t) \\right) .\n",
    "$$\n",
    "\n",
    "**Task**: in this exercise, you will replace the SARSA update rule by the Q-learning one and analyze the\n",
    "differences in performances.\n",
    "\n",
    "---\n",
    "QLearning\n",
    "---\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "Initialize arbitrarily $Q(\\boldsymbol{s}, \\boldsymbol{a}) ~~~ \\forall \\boldsymbol{s} \\in \\mathcal{S}, \\boldsymbol{a} \\in \\mathcal{A}(\\boldsymbol{s})$<br>\n",
    "$Q(\\boldsymbol{s}_F, \\cdot) = 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (initialize finale states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $S \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $A \\leftarrow \\epsilon\\text{-greedy}(S, Q)$<br>\n",
    "\t\t$\\quad\\quad$ $R, S' \\leftarrow \\text{env.step}(A)$<br>\n",
    "\t\t$\\quad\\quad$ $Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ \\underbrace{R + \\gamma ~ \\max_{\\boldsymbol{a}} Q(S', \\boldsymbol{a}) ~ - ~ Q(S,A)}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $S \\leftarrow S'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $S$ is final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ef3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "# Initialize the history of the Q-table and learning rate\n",
    "q_array_history_ex5 = []\n",
    "alpha_history_ex5 = []\n",
    "\n",
    "def q_learning(\n",
    "    environment: gym.Env, \n",
    "    alpha: float = 0.1, \n",
    "    alpha_factor: float = 0.9995, \n",
    "    gamma: float = 0.99, \n",
    "    epsilon: float = 0.5, \n",
    "    num_episodes: int = 10000, \n",
    "    display: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform Q-learning on a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    environment : gym.Env\n",
    "        The environment to learn in.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor to decrease alpha by each episode, by default 0.9995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.99\n",
    "    epsilon : float, optional\n",
    "        The probability of choosing a random action, by default 0.5\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 10000\n",
    "    display : bool, optional\n",
    "        Whether to display the Q-table (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The learned Q-table.\n",
    "        Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    \"\"\"\n",
    "    # Get the number of states and actions in the environment\n",
    "    num_states = environment.observation_space.n\n",
    "    num_actions = environment.action_space.n\n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    q_array = np.zeros([num_states, num_actions])\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in range(num_episodes):\n",
    "        # Display the Q-table every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            qtable_display(q_array, title=\"Q table\")\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "\n",
    "        # Save the current Q-table and learning rate\n",
    "        q_array_history_ex5.append(q_array.copy())\n",
    "        alpha_history_ex5.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    # Return the learned Q-table\n",
    "    return q_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ddb91f",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the action-value function `q_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989cf3d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "# Initialize the history of the action-value function and learning rate\n",
    "q_array_history_ex5.clear()\n",
    "alpha_history_ex5.clear()\n",
    "\n",
    "# Apply Q-learning to calculate the Q-table for the FrozenLake environment\n",
    "q_array = q_learning(environment, display=False)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the learned Q-table\n",
    "qtable_display(q_array, title=\"Q Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a50e06",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Pandas dataframe containing the evolution of the Q-table during the learning process\n",
    "q_array_history_array_ex5 = np.array(q_array_history_ex5)\n",
    "df_q_hist_list_ex5 = []\n",
    "\n",
    "for action_index in range(q_array_history_array_ex5.shape[2]):\n",
    "    df_q_hist_list_ex5.append(pd.DataFrame(q_array_history_array_ex5[:, :, action_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7cd50",
   "metadata": {},
   "source": [
    "Evolution of `q_array` over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95526d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the estimated expected value for each action and state during the learning process\n",
    "for action_index, df_q_hist in enumerate(df_q_hist_list_ex5):\n",
    "    df_q_hist.plot(figsize=(14,8))\n",
    "    plt.title(r'$Q(\\cdot,a_{})$ w.r.t iteration with $a_{}$ := \"{}\"'.format(action_index, action_index, action_labels[action_index]))\n",
    "    plt.ylabel(r\"$Q(\\cdot,a_{})$\".format(action_index))\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbb8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the alpha parameter during the learning process\n",
    "plt.loglog(alpha_history_ex5)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cecdc4d",
   "metadata": {},
   "source": [
    "### Evaluate Policy with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee2c11",
   "metadata": {},
   "source": [
    "As a measure of performance, count the number of successful trials on 1000 episodes.\n",
    "\n",
    "**Note**: Gymnasium considers the task is solved if you reach 76\\% of success over the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ccdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('FrozenLake-v1')\n",
    "environment._max_episode_steps = 1000\n",
    "\n",
    "reward_list = []\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "    done = False\n",
    "    #t = 0\n",
    "\n",
    "    while not done:\n",
    "        #action = epsilon_greedy_policy(state, q_array, epsilon)\n",
    "        action = greedy_policy(state, q_array)\n",
    "        state, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "reward_df = pd.DataFrame(reward_list)\n",
    "\n",
    "print('Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):', np.average(reward_df))\n",
    "\n",
    "environment.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
